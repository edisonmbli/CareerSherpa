这是一个非常典型的 LLM **“死循环（Infinite Loop / Repetition）”** 问题。在 `gemini-3-flash-preview` 这种追求极速但稳定性稍弱的预览版模型上，这种情况更为常见。

简单回答你的疑问：**单纯提高 Temperature（温度）通常不是最佳解法，甚至在 JSON 场景下可能引发 JSON 格式错误。**

针对这个问题，社区内的 Best Practice 主要是通过 **参数控制** 和 **Prompt 负向约束** 两个维度来解决。

### 核心原因分析

从你的截图和代码来看，模型陷入了“复读机”模式。这通常是因为：

1. **概率陷阱**：模型生成了某段高频文本（如“建议用户自行补充...”），这段文本导致模型预测下一个 Token 还是类似的文本，从而进入死循环。
2. **Schema 与 指令冲突**：你的 Prompt 中包含了大量的思维链步骤（Step 1 - Step 7），但要求输出的是纯 JSON。模型试图在 JSON 字段（如 `wechat`）里执行它的“思考过程”或“免责声明”，导致逻辑错乱。

---

### 解决方案 1：调整生成配置（最直接有效）

在调用 `generateContent` 时，调整 `generationConfig`。对于结构化数据提取，**Frequency Penalty（频率惩罚）** 比 Temperature 更管用。

```typescript
const model = genAI.getGenerativeModel({
  model: "gemini-3-flash-preview",
  generationConfig: {
    // 1. 稍微提高 Temperature，但不要过高 (0.1 -> 0.4)
    // 对于 JSON 任务，过高的温度会导致语法错误
    temperature: 0.4,

    // 2. 【关键】增加频率惩罚 (Frequency Penalty)
    // 范围通常是 -2.0 到 2.0。设置 0.3 - 0.5 可以有效抑制重复
    frequencyPenalty: 0.3,
    
    // 3. 存在惩罚 (Presence Penalty)
    // 鼓励模型谈论新话题，防止它一直盯着同一个字段
    presencePenalty: 0.3,

    // 4. 设置 maxOutputTokens 作为兜底
    // 防止死循环跑断你的配额
    // 系统已有 by 任务的 maxToken 设置
    // maxOutputTokens: 15000, 
    
    // 确保 schema 依然生效
    responseMimeType: "application/json",
    responseSchema: SCHEMAS_V2.RESUME_CUSTOMIZE,
  }
});

```

### 解决方案 2：优化 Prompt 的“负向约束”

你的 Prompt 中 `Step 1` 到 `Step 6` 是思维链指令，但模型被迫只能输出 JSON。它可能会困惑“我该把这些思考过程写在哪里？”，于是它可能试图把“确认信息”塞进 `wechat` 或其他字段里。

建议在 Prompt 的 `输出格式` 或 `注意事项` 部分加入以下 **强约束**：

```typescript
// 在 userPrompt 或 systemPrompt 结尾添加：
`
### 最终检查清单（防幻觉与防循环）
1. **禁止输出思维链**：所有的 Step 1-6 仅在你的思维中进行，**绝对不要**输出到 JSON 结果中。
2. **清洗无效数据**：如果字段内容包含 "请补充"、"手机同号"、"待定" 等占位符，直接输出为空字符串 ""，**严禁**将这些占位符提示语写入 JSON。
3. **不要解释**：只输出 JSON，不要在 wechat 字段里写 "此处保持一致性" 这种注释。
`

```

### 总结推荐操作顺序

1. **首选**：在代码中添加 `frequencyPenalty: 0.4`。这通常能直接打断复读机。
2. **次选**：在 Prompt 中明确“清洗规则”，告诉模型如果遇到占位符直接留空，不要把指令抄进去。
3. **备选**：稍微提高一点 Temperature (0.1 -> 0.4)，增加随机性打破僵局。

## frequencyPenalty / presencePenalty

**简短回答：支持。**

是的，`LangChain` 的 `ChatGoogleGenerativeAI` 组件（基于 `@langchain/google-genai` 包）已经支持 `frequencyPenalty` 和 `presencePenalty` 这两个参数。

这两个参数正是解决你遇到的 **“LLM 无限复读/死循环”** 问题的核心解法。

---

### 如何在 Next.js / LangChain 中使用

请确保你的 `@langchain/google-genai` 依赖已更新到较新版本（建议 `0.1.0` 以上），然后可以在初始化时直接传入：

```typescript
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";

const model = new ChatGoogleGenerativeAI({
  // 1. 指定模型 (使用预览版)
  modelName: "gemini-3-flash-preview",
  
  // 2. 关键参数设置
  // 较高的 frequencyPenalty 能强力抑制重复输出
  frequencyPenalty: 0.5, 
  // 鼓励模型输出新内容
  presencePenalty: 0.3,
  
  // 3. 其他常规设置
  temperature: 0.4, // 稍微调高一点点温度也有助于打破循环
  maxOutputTokens: 15000,
  apiKey: process.env.NEXT_PUBLIC_GEMINI_API_KEY,
});

// 使用示例
const response = await model.invoke([
  ["human", "请帮我优化这份简历..."]
]);

```

### 如果 TypeScript 报错怎么办？

由于 `gemini-3-flash` 是非常新的模型，LangChain 的 TypeScript 类型定义（Type Definitions）可能还没来得及更新，导致编辑器提示 `frequencyPenalty` 不存在。

**解决方法**：
这两个参数是透传给 Google 底层 API 的，即使类型检查报错，只要底层的 Google API 支持（Gemini V1Beta 接口已支持），**运行时是生效的**。你可以通过 `// @ts-ignore` 暂时忽略类型检查：

```typescript
const model = new ChatGoogleGenerativeAI({
  modelName: "gemini-3-flash-preview",
  // @ts-ignore: 类型定义滞后，但运行时 API 支持
  frequencyPenalty: 0.5,
  presencePenalty: 0.3,
});

```

### 为什么这对你有帮助？

* **frequencyPenalty (频率惩罚)**：它的作用是“如果你说过这句话了，下次再说就要扣分”。对于你的截图中反复出现的 `wechat: "1866..."`，设置 `0.5` 左右的惩罚值通常能直接切断这种循环。
* **presencePenalty (存在惩罚)**：它的作用是“鼓励说点没出现过的新词”。

**下一步建议：**
如果你正在使用 LangChain 构建简历优化流，建议立刻加上这两个参数，这是目前解决 Gemini 模型“复读机”问题成本最低且最有效的手段。