** M6：异步基建 - LLM 调度器的落地指南 **

# 1. 难题描述

## 如何“握手”？

当前队列设计是异步架构的，即调用者（server action）把任务载荷（Payload）推送到选定的 QStash 队列后，便立即向前端返回 HTTP 200。而 LLM 任务执行的时间都比较长，中间是如何完成握手传递，当 LLM 开始返回 or 完整返回结果时，能及时、有效地把结果呈现到用户面前（前端页面）呢？这里的机制细节我没有想明白，请详细深入讲解下，尤其是针对以下两个典型场景：

- 流式（stream mode），譬如，工作匹配度分析、面试定向经验这两个任务
- 非流式，譬如，resume/detailed_resume/job 的 summary 提取、简历定制化

## Stream Mode 更为复杂

尤其是流式场景，比较复杂，要考虑的细节有：

1. 如果不推送 QStash 的话，也就是不会影响背压 counter，那么并发控制就靠每个模型（队列）的 kv 并发锁来把关，确保同时运行的任务（含队列和直接提交）不超过 api 上限吗

2. 但上述方案是过于理想了，因地制宜来看，不能在本次项目中落地：

- chatGPT 能做到像”秒回”的体验，背后堆了大量的资源才能把 C 端体验做好；而我们只是 MVP 阶段的初创项目，云服务器资源有限、API 并发量也少得可怜
- 在此资源困境下，如果允许直接提交任务，必然造成 LLM 模型资源的哄抢；而在稍微繁忙的时间点，大部分用户都抢不到锁，因此前端的体验就是 - 任务提交失败，请稍后再“抢”，这样的体感太差了

3. 如果直接提交任务不可行，那么回到队列管控模式是折中的选择？但是走队列的这个方案，很多细节需要盘算清楚，包括但不限于：

- **用户体验如何保障？** 从用户提交到结果开始返回，不可避免地会有明显延时（快则 10 几秒、慢 1 ～ 2 分钟），可预期用户会有 3 种行为模式：

  - Case 1. 很着急，就在页面等候 - 这种情况，如何缓解用户焦虑的情绪呢？状态更新（处理中、排队中），预期管理（预计还需要 xx 分钟 或 xx 分钟以上）；而当 LLM 开始返回结果时，如何第一时间从 worker 手里拿到 streaming 结果，及时展示到页面？

  - Case 2. 有小等预期，短暂切换页面先干其它，晚点再切换回来看结果 - 这种情况，处理上是不是和上述 case 1 一致？只是对非 active 的页面保持状态/结果的实时更新？

  - Case 3. 没有耐心，直接关闭当前页面，过段时间再回来访问结果 - 这种情况不涉及页面的实时更新，但其实也很复杂，因为当前任务可能还在处理中、也可能已经处理完成了。在用户离开期间，后台要如何接管 streaming 的任务、并在合适的时机写库呢？而针对不同的任务进展，用户回来重新打开页面的交互应当如何设计呢？

- ** 资源抢占的困境怎么破？** 因为 streaming 类的任务时效要求高，很自然会想到，把这类任务的优先级提高，但这样做恐怕是不行的，因为试想下，繁忙时期大量服务类请求会直接把同期的资产类请求阻塞掉（虽然一直在队列里，可是因为资产类的优先级低，就一直被插队）；但如果为不同的任务 x 模型都建立独立队列的话，那么队列数量将直接膨胀，长期不好运维且容易引出奇怪的 bug。只能接受按先来后到的方式，在同队列内串行搞吗？

- ** 跨队列的复杂任务怎么搞？** job_match 是本项目最复杂的任务场景，可以想象下最难搞的情况：用户上传岗位截图，提交匹配度分析 → vision 提取图片文字（队列 A）→ 文字提取 job summary（队列 B）→ 查询 RAG、查询用户资产、构建 prompt、调用 LLM（队列 C ）→ 按用户行为模式把结果用合适的方式返回

# 2. 解题思路

## 平衡“有限资源”与“流式体验”，让 QStash 成为唯一的“阀门”

核心架构原则 (Core Principle)： 所有 LLM 任务（无论流式还是非流式），必须 100% 通过 QStash 队列提交。绝不允许 Server Action "插队"直连 LLM 资源。

不是“一套方案用到底”，而是针对两种截然不同的用户场景，设计了两套不同的“握手”机制。

### 场景 1：非流式任务 (Non-Streaming)

- 任务：resume_summary, detailed_resume_summary, customize。
- 用户心智：用户不期望在页面上“干等”30 秒。他希望提交任务后，先去做别的事（甚至关闭浏览器），稍后再回来查看结果。
- 握手机制：异步队列 (QStash) + 状态轮询 (Polling)。
- 详细流程 (AI IDE 必须在 M8/M9 中实现)：

1. [前端 M8]：用户在 Profile Page 点击“上传简历”。
2. [后端 M8 Server Action]：
   (M2) 检查金币、(M6) 检查背压、(M6) 扣费/降级。
   (M1) 在 Resumes 表中创建一条记录，status: 'PENDING'，获取 resume.id。
   (M6.3) 推送：调用 M6 的 QStash Producer，将 { resumeId: resume.id, ... } 推送到队列。
   (立即返回)：return { success: true, taskId: resume.id, taskType: 'resume' }。
3. [前端 M8]：
   收到 { taskId: ... }。
   (握手)：启动“智能轮询”，模拟退火，每 20/10/5/3 秒调用一次 M2 的 GET /api/task-status 接口。
4. [后端 M8 QStash Worker]：
   (M6) 获取并发锁。
   (M4) 调用 runStructuredLlmTask (获取 JSON)。
   (M1) 将结果和 status: 'COMPLETED' 写回 Resumes 表。
   (M6) 释放锁/计数器。
5. [前端 M8]：api/task-status 返回 { status: 'COMPLETED' }。前端停止轮询，刷新页面数据，显示“成功” toast。

### 场景 2：流式任务 (Streaming)

- 任务：job_match, interview_prep。
- 用户心智：用户正在屏幕前“积极等待”。他要的是尽快看到结果（像 ChatGPT 一样），asap。
- 握手机制：同步流式响应 (Synchronous Streaming)，搭配使用 QStash Redis 的 Pub/Sub (发布/订阅) 功能，配合 Server-Sent Events (SSE) 来搭桥。
- 详细流程 (AI IDE 必须在 M8/M9 中实现)：

1. [前端 M9]：用户点击“开始分析”。
2. [前端 M9]：浏览器立即向 Vercel 打开一个持久的 GET /api/sse-stream 连接。这个连接会保持 Keep-Alive。
3. [后端 SSE]：sse-stream 路由（AI IDE M6.5 将创建）收到请求，它立即订阅一个特定于该用户的 Redis 频道（例如 KV.subscribe('stream:USER_ID')）。然后它就“挂起”在那里等待。
4. [后端 M9 Server Action]：
   (并行) Server Action 检查金币、背压、扣费。
   (推送) 它将 job_match 任务推送到 QStash 队列（例如 q_paid_stream）。
   (立即返回) return { success: true, taskId: ... }。
5. [前端 M9]：
   收到 success。它不需要轮询。
   它唯一要做的就是显示“排队中...”（Queued）。
6. [后端 M9 QStash Worker]：
   （10 秒... 60 秒... 120 秒后）Worker 终于从 QStash 拿到任务。
   (M6) 获取并发锁。
   (M4) 调用 runStreamingLlmTask。
   (关键握手)：LLM 每返回一个 token（例如 "匹"、"配"、"度"），Worker 立即将这个 token 发布 (PUBLISH) 到 KV 的 stream:USER_ID 频道。
7. [后端 SSE]：sse-stream 路由（在步骤 3 中）立即收到了这个 token。
8. [后端 SSE]：它立即将 token 写入 SSE 响应流，发回给前端。
9. [前端 M9]：前端（在步骤 2 中）的 SSE 连接收到了 token，并将其渲染到 Card 上。

### 这个方案如何解决上述难题的 3 个 Case？

- Case 1 (等待)：完美解决。前端显示“排队中...”，一旦 Worker 启动，token 就会自动流式出现，焦虑感消失。
- Case 2 (切走)：完美解决。SSE 连接在现代浏览器中（非 active 标签页）会保持，切换回来时内容已更新。
- Case 3 (关闭)：
  - 握手失败：sse-stream 连接断开。
  - Worker 降级：当 Worker (M6.7) 尝试向 KV.publish() 时，它会发现“订阅者数量”为 0。
  - Worker 职责：此时，Worker 必须停止推送 token，转而将 LLM 的完整结果拼接起来，存入数据库（例如 matches.matchSummaryJson），并更新 status: 'COMPLETED'。
  - 用户返回：当用户“过段时间再回来”，M8 的 Workbench 会从“历史记录”中加载 service，发现 matches 表已有数据，于是直接渲染，而不是建立 SSE 连接。

## 资源抢占（队列）策略

“优先级插队”的思路不可行，我们不能让资产任务（resume_summary）饿死。
答案： 我们将 M4 的 task-router.ts 升级。我们不按“任务类型”分，而是按“体验模式”+“付费等级”来分。

修正后的 QueueId (M4 重构)：

- q_paid_stream：用于 match, interview（付费用户）。高并发。
- q_paid_batch：用于 resume, customize（付费用户）。中并发。
- q_free_stream：用于 match, interview（免费用户）。低并发。
- q_free_batch：用于 resume, customize（免费用户）。最低并发。

为什么这能行？ Upstash QStash 会并行处理这 4 个队列。q_paid_stream 上的任务不会被 q_free_batch 上的任务阻塞。我们的 M6.1 KV 并发锁是**按模型（ModelId）**设置的，这确保了所有 4 个队列共享“deepseek-chat 并发池”，实现了公平抢占。

## 跨队列复杂任务

本项目最复杂的“噩梦场景”（OCR -> Summary -> Match） 是我们新架构必须解决的。
答案： ** QStash 任务链 (Worker Chaining) **。通过“轮询 (Polling) -> 流 (Streaming)”的混合切换机制，完美解决了“复杂长任务链”和“流式体验”的终极矛盾。

- 详细流程 (AI IDE 必须在 M9 中实现)：

1. [M9 Server Action]：
   推送 Task A (job-ocr-worker) 到 q_paid_vision 队列。
   立即返回 { taskId: ..., status: 'OCR_PENDING' }。
2. [前端 M9]：
   启动轮询 (GET /api/task-status)。
   显示 "正在识别岗位截图..."
3. [M9 Worker A (OCR)]：
   完成 OCR -> 拿到 raw_text。
   (关键) 不返回，而是 (M6.3) 推送 Task B (job-summary-worker) 到 q_paid_batch，payload 中包含 raw_text。
   (M1) 更新 DB Jobs 表 status: 'SUMMARY_PENDING'。
4. [前端 M9]：轮询 task-status，发现状态变为 SUMMARY_PENDING，更新 UI：“正在提取岗位要点...”。
5. [M9 Worker B (Summary)]：
   完成 job_summary -> 拿到 summary_json。
   (关键) 不返回，而是 (M6.3) 推送 Task C (job-match-worker) 到 q_paid_stream。
   (M1) 更新 DB Jobs 表 status: 'MATCH_PENDING'。
6. [前端 M9]：轮询 task-status，发现状态变为 MATCH_PENDING。
   (关键握手)：停止轮询，立即建立 GET /api/sse-stream 连接。
   更新 UI：“已进入匹配队列，请稍候...”
7. [M9 Worker C (Match)]：
   (M6.7) 启动，开始流式处理。
   (M6.5) 通过 KV.publish() 将 token 推送到 sse-stream。
8. [前端 M9]：开始实时接收流式结果。

# 3. M6 落地指南

- 里程碑目标：构建 Redis 和 QStash 基础设施，为后续 M8（资产流）和 M9（服务流）提供“并发控制”和“任务队列”能力。
- 核心规范：遵循 project rules（Rule 9） 和 M0 确认的 Upstash 技术栈。

#### 任务 0：(AI IDE) 安装依赖 (M0 已完成)

- **AI IDE 动作**：
  ```bash
  # Upstash Redis
  pnpm add @upstash/redis
  # Upstash QStash
  pnpm add @upstash/qstash
  # (M8/M9 需要) Vercel AI SDK (用于 SSE)
  pnpm add ai
  ```

#### 任务 1：(AI IDE) 升级 M4 的 `task-router.ts`

- **目标**：实现 M6 新的“体验模式”队列。
- **文件**：`lib/llm/task-router.ts` (AI IDE 必须**覆盖**此文件)

<!-- end list -->

```typescript
// AI IDE: Overwrite this file at lib/llm/task-router.ts
import type { TaskTemplateId } from '@/lib/prompts/types'
import { ModelId } from './providers'

// 规范: M6 队列按“体验模式”+“付费等级”划分
export enum QueueId {
  PAID_STREAM = 'q_paid_stream',
  PAID_BATCH = 'q_paid_batch',
  PAID_VISION = 'q_paid_vision',

  FREE_STREAM = 'q_free_stream',
  FREE_BATCH = 'q_free_batch',
  FREE_VISION = 'q_free_vision',
}

interface TaskRouting {
  modelId: ModelId
  queueId: QueueId
  isStream: boolean // 标记此任务是否应流式
}

const ROUTING_TABLE: Record<
  TaskTemplateId,
  { paid: TaskRouting; free: TaskRouting }
> = {
  // --- M7 资产流 (Batch) ---
  resume_summary: {
    paid: {
      modelId: ModelId.DEEPSEEK_CHAT,
      queueId: QueueId.PAID_BATCH,
      isStream: false,
    },
    free: {
      modelId: ModelId.GLM_4_5_FLASH,
      queueId: QueueId.FREE_BATCH,
      isStream: false,
    },
  },
  detailed_resume_summary: {
    paid: {
      modelId: ModelId.DEEPSEEK_REASONER,
      queueId: QueueId.PAID_BATCH,
      isStream: false,
    },
    free: {
      modelId: ModelId.GLM_4_5_FLASH,
      queueId: QueueId.FREE_BATCH,
      isStream: false,
    },
  },
  job_summary: {
    // JD 文本摘要 (Batch)
    paid: {
      modelId: ModelId.DEEPSEEK_CHAT,
      queueId: QueueId.PAID_BATCH,
      isStream: false,
    },
    free: {
      modelId: ModelId.GLM_4_5_FLASH,
      queueId: QueueId.FREE_BATCH,
      isStream: false,
    },
  },

  // --- M8/M9 服务流 (Stream) ---
  job_match: {
    paid: {
      modelId: ModelId.DEEPSEEK_REASONER,
      queueId: QueueId.PAID_STREAM,
      isStream: true,
    },
    free: {
      modelId: ModelId.GLM_4_5_FLASH,
      queueId: QueueId.FREE_STREAM,
      isStream: true,
    },
  },
  interview_prep: {
    paid: {
      modelId: ModelId.DEEPSEEK_CHAT,
      queueId: QueueId.PAID_STREAM,
      isStream: true,
    },
    free: {
      modelId: ModelId.GLM_4_5_FLASH,
      queueId: QueueId.FREE_STREAM,
      isStream: true,
    },
  },

  // --- M9 服务流 (Batch) ---
  resume_customize: {
    paid: {
      modelId: ModelId.DEEPSEEK_CHAT,
      queueId: QueueId.PAID_BATCH,
      isStream: false,
    },
    free: {
      modelId: ModelId.GLM_4_5_FLASH,
      queueId: QueueId.FREE_BATCH,
      isStream: false,
    },
  },
}

/**
 * M8/M9 Server Action 调用的核心函数
 */
export const getTaskRouting = (
  templateId: TaskTemplateId,
  hasQuota: boolean
) => {
  const route = ROUTING_TABLE[templateId]
  if (!route) throw new Error(`Routing for task ${templateId} not found.`)
  return hasQuota ? route.paid : route.free
}

// M9 特殊任务：JD 截图
export const getJobVisionTaskRouting = (hasQuota: boolean): TaskRouting => {
  return hasQuota
    ? {
        modelId: ModelId.GLM_4_VISION,
        queueId: QueueId.PAID_VISION,
        isStream: false,
      }
    : {
        modelId: ModelId.GLM_4_VISION,
        queueId: QueueId.FREE_VISION,
        isStream: false,
      }
}
```

#### 任务 2 & 3：KV 并发锁 & 背压计数器 (M6.1, M6.2)

- **AI IDE 动作**：

  1.  创建 `lib/kv/client.ts` (使用 `@upstash/redis`)。

  - 目标：创建一个 Redis 客户端单例。
  - 文件：lib/kv/client.ts

  ```typescript
  // AI IDE: Create this file at lib/kv/client.ts
  import { Redis } from '@upstash/redis'

  // 遵循 Rule 9 (Upstash) 和 M0 规范
  export const kv = new Redis({
    url: process.env.UPSTASH_REDIS_REST_URL!,
    token: process.env.UPSTASH_REDIS_REST_TOKEN!,
  })
  ```

  2.  创建 `lib/kv/lock.ts` (实现 `acquireConcurrencyLock`, `releaseConcurrencyLock`)。

  - 目标：实现一个基于 Redis 的分布式锁，用于所有 LLM 任务（同步流式和异步队列），防止 API 密钥超并发。
  - 文件：lib/kv/lock.ts

  ```typescript
  // AI IDE: Create this file at lib/kv/lock.ts
  import { kv } from './client'

  const LOCK_TTL_SECONDS = 300 // 锁的最长存活时间 (5分钟)，防止死锁

  /**
   * 尝试获取一个并发锁
   * @param resourceId 要锁定的资源 (例如 'glm-4.5-flash')
   * @param maxConcurrency 最大并发数 (例如 2)
   * @returns { success: boolean, lockKey: string | null }
   */
  export async function acquireConcurrencyLock(
    resourceId: string,
    maxConcurrency: number
  ) {
    const key = `lock:${resourceId}`

    // 1. 获取当前并发数
    const currentConcurrency = await kv.scard(key)

    if (currentConcurrency >= maxConcurrency) {
      return { success: false, lockKey: null } // 达到并发上限
    }

    // 2. 尝试获取锁
    const lockKey = `${key}:${Date.now()}:${Math.random()}`
    await kv.sadd(key, lockKey)
    await kv.expire(key, LOCK_TTL_SECONDS) // 设置 TTL，防止死锁

    return { success: true, lockKey }
  }

  /**
   * 释放一个并发锁
   */
  export async function releaseConcurrencyLock(
    resourceId: string,
    lockKey: string
  ) {
    const key = `lock:${resourceId}`
    await kv.srem(key, lockKey)
  }
  ```

  3.  创建 `lib/kv/counter.ts` (实现 `incrementQueueCounter`, `decrementQueueCounter`)。

  - 目标：实现一个基于 Redis 的原子计数器，用于异步队列（QStash），防止任务积压。
  - 文件：lib/kv/counter.ts

  ```typescript
  // AI IDE: Create this file at lib/kv/counter.ts
  import { kv } from './client'

  const COUNTER_TTL_SECONDS = 3600 // 计数器 1 小时后自动过期

  /**
   * 尝试增加队列计数器 (背压检查)
   * @param queueId 队列 ID (例如 'q_glm_flash')
   * @param maxLength 队列最大长度
   * @returns { success: boolean }
   */
  export async function incrementQueueCounter(
    queueId: string,
    maxLength: number
  ): Promise<{ success: boolean; current: number }> {
    const key = `counter:${queueId}`

    // 原子化增加
    const current = await kv.incr(key)

    if (current === 1) {
      // 如果是第一个，设置 TTL
      await kv.expire(key, COUNTER_TTL_SECONDS)
    }

    if (current > maxLength) {
      // 超过上限，回滚
      await kv.decr(key)
      return { success: false, current: current - 1 }
    }

    return { success: true, current }
  }

  /**
   * 减少队列计数器 (Worker 完成时调用)
   */
  export async function decrementQueueCounter(queueId: string) {
    const key = `counter:${queueId}`
    await kv.decr(key)
  }
  ```

#### 任务 4：QStash 生产者 (M6.3)

- **目标**：创建 `lib/queue/producer.ts`。
- **AI IDE 动作**：
  1.  创建 `lib/queue/producer.ts`。
  2.  代码与上一轮 M6 指南中的**几乎相同**，但 `pushTaskToQueue` 的 `payload` 需要更标准化，并**返回 `taskId`**。
- **文件**：`lib/queue/producer.ts` (精简版)

<!-- end list -->

```typescript
// AI IDE: Create this file at lib/queue/producer.ts
import { Client } from '@upstash/qstash'
import { QueueId } from '@/lib/llm/task-router'
import { TaskTemplateId } from '@/lib/prompts/types'
import { Locale } from '@/i18n-config'

// ... (qstashClient 和 VERCEL_URL 定义) ...
const qstashClient = new Client({ token: process.env.UPSTASH_QSTASH_TOKEN! })
const VERCEL_URL = process.env.VERCEL_URL
  ? `https://${process.env.VERCEL_URL}`
  : 'http://localhost:3000'

export interface QStashPayload {
  taskTemplateId: TaskTemplateId
  modelId: string
  locale: Locale
  context: {
    userId: string
    serviceId?: string
    // 任务ID，例如 resume.id 或 customizedResume.id
    // M7/M9 将使用此 ID 来轮询
    taskId: string
    [key: string]: any
  }
}

/**
 * M6.3 核心：向 QStash 推送一个异步任务
 * @param queueId 目标队列
 * @param workerName Worker API 路由 (例如 'resume-worker')
 * @param payload 任务载荷
 */
export async function pushTaskToQueue(
  queueId: QueueId,
  workerName: string,
  payload: QStashPayload,
  dependencies: { dependsOn?: string } = {}
) {
  const destinationUrl = `${VERCEL_URL}/api/worker/${workerName}`

  try {
    const response = await qstashClient.publishJSON({
      url: destinationUrl,
      queue: queueId, //
      body: payload,
      retries: 3,
      ...dependencies,
    })

    return { success: true, messageId: response.messageId }
  } catch (error) {
    console.error(`[QStash Producer] Failed to push to ${queueId}:`, error)
    return { success: false, error: (error as Error).message }
  }
}
```

#### 任务 5：实现“异步握手”SSE 路由 (M6.5)

- **目标**：**(解决 困惑 3a)** 创建 `GET /api/sse-stream` 路由，用于将 KV Pub/Sub 的消息流式传输到前端。
- **文件**：`app/api/sse-stream/route.ts`

<!-- end list -->

```typescript
// AI IDE: Create this file at app/api/sse-stream/route.ts
import { kv } from '@/lib/kv/client'
import { auth } from '@job-assistant/authkit'

// 规范：必须使用 Edge Runtime 才能支持 KV Pub/Sub
export const runtime = 'edge'

// 这是一个长连接
export async function GET(request: Request) {
  // 1. (M2) 认证：必须通过 auth 检查
  const session = await auth()
  if (!session || !session.userId) {
    return new Response('Unauthorized', { status: 401 })
  }
  const { userId } = session

  // 2. (M6.5) 建立流
  const stream = new ReadableStream({
    async start(controller) {
      const encoder = new TextEncoder()

      // 向客户端发送 "heartbeat" 确认连接
      controller.enqueue(encoder.encode(': connected\n\n'))

      // 3. (M6.5) 订阅该用户的 KV 频道
      const pubsub = kv.subscribe(`stream:${userId}`)
      console.log(`SSE: User ${userId} subscribed to KV Pub/Sub.`)

      for await (const message of pubsub) {
        // 4. (M6.7) 从 Worker 收到 token，将其转发给前端
        const data = `data: ${JSON.stringify(message)}\n\n`
        controller.enqueue(encoder.encode(data))
      }
    },
    async cancel() {
      // 5. 客户端断开连接
      console.log(`SSE: User ${userId} disconnected.`)
      await kv.unsubscribe()
    },
  })

  return new Response(stream, {
    headers: {
      'Content-Type': 'text/event-stream',
      Connection: 'keep-alive',
      'Cache-Control': 'no-cache, no-transform',
    },
  })
}
```

#### 任务 6 & 7：创建 QStash Workers (M6.6, M6.7)

- **目标**：创建两种类型的 Worker 骨架，M8/M9 将基于此填充业务。
- **文件 6.1 (Batch)**：`app/api/worker/batch-processor/route.ts`

<!-- end list -->

```typescript
// AI IDE: Create this file at app/api/worker/batch-processor/route.ts
// 这是 M8 (resume、detailed_resume) 和 M9 (customize) 的 Worker 模板
import { NextResponse } from 'next/server'
import { verifySignatureEdge } from '@upstash/qstash/dist/nextjs'
import { runStructuredLlmTask } from '@/lib/llm/service' // M4
import { decrementQueueCounter } from '@/lib/kv/counter' // M6.2
import { acquireConcurrencyLock, releaseConcurrencyLock } from '@/lib/kv/lock' // M6.1
import { QStashPayload } from '@/lib/queue/producer'
import { db } from '@/lib/prisma'
import { addQuota } from '@/lib/dal/quotas' // M2

export const runtime = 'edge'

async function handler(request: Request) {
  const payload = (await request.json()) as QStashPayload
  const { taskTemplateId, modelId, locale, context } = payload
  const { userId, taskId, queueId } = context // queueId 需在 M8 传入

  let lockKey: string | null = null

  try {
    const { success, lockKey: acqKey } = await acquireConcurrencyLock(
      modelId,
      5
    ) // 假设并发 5
    if (!success) {
      return NextResponse.json(
        { error: 'Concurrency limit reached' },
        { status: 429 }
      )
    }
    lockKey = acqKey

    // (M4) 调用非流式 LLM
    const structuredResult = await runStructuredLlmTask(
      modelId as any,
      taskTemplateId,
      locale,
      { ...context }, // 传入 M8 所需变量
      { userId, serviceId: context.serviceId }
    )

    // (M8) 更新数据库
    // (AI IDE: M8 将在此处添加
    // e.g., db.resume.update({ where: { id: taskId }, data: { ...structuredResult, status: 'COMPLETED' } })
    console.log(`[BatchWorker] Task ${taskId} completed.`)

    return NextResponse.json({ success: true })
  } catch (error) {
    // (M7) 更新 DB 状态为 FAILED
    // e.g., db.resume.update({ where: { id: taskId }, data: { status: 'FAILED' } })

    // (M2) 金币返还
    // const cost = config.costs[taskTemplateId];
    // await addQuota(userId, cost);

    return NextResponse.json(
      { error: (error as Error).message },
      { status: 500 }
    )
  } finally {
    // (M6) 释放锁和计数器
    if (lockKey) await releaseConcurrencyLock(modelId, lockKey)
    if (queueId) await decrementQueueCounter(queueId)
  }
}
export const POST = verifySignatureEdge(handler)
```

- **文件 6.2 (Stream)**：`app/api/worker/stream-processor/route.ts`

<!-- end list -->

```typescript
// AI IDE: Create this file at app/api/worker/stream-processor/route.ts
// 这是 M9 (match) 和 M9 (interview) 的 Worker 模板
import { NextResponse } from 'next/server'
import { verifySignatureEdge } from '@upstash/qstash/dist/nextjs'
import { runStreamingLlmTask } from '@/lib/llm/service' // M4
import { decrementQueueCounter } from '@/lib/kv/counter' // M6.2
import { acquireConcurrencyLock, releaseConcurrencyLock } from '@/lib/kv/lock' // M6.1
import { QStashPayload } from '@/lib/queue/producer'
import { db } from '@/lib/prisma'
import { addQuota } from '@/lib/dal/quotas' // M2
import { kv } from '@/lib/kv/client' // M6.1
import { LangChainStream } from 'ai'

export const runtime = 'edge'

async function handler(request: Request) {
  const payload = (await request.json()) as QStashPayload
  const { taskTemplateId, modelId, locale, context } = payload
  const { userId, taskId, queueId, serviceId } = context
  const kvChannel = `stream:${userId}:${serviceId}` // 唯一的 KV 频道

  let lockKey: string | null = null

  try {
    const { success, lockKey: acqKey } = await acquireConcurrencyLock(
      modelId,
      5
    )
    if (!success) {
      return NextResponse.json(
        { error: 'Concurrency limit reached' },
        { status: 429 }
      )
    }
    lockKey = acqKey

    // (M9) 更新 DB 状态，通知前端“停止轮询，准备接收SSE”
    // e.g., db.match.update({ where: { id: taskId }, data: { status: 'STREAMING' } })

    let fullOutput = ''

    // (M4) 调用流式 LLM
    const stream = await runStreamingLlmTask(
      modelId as any,
      taskTemplateId,
      locale,
      { ...context },
      { userId, serviceId },
      {
        // (M6.7) 流式结束后的回调
        onStreamEnd: async (output, usage) => {
          fullOutput = output
          // (M9) 异步将完整结果写回 DB
          // e.g., db.match.update({ where: { id: taskId }, data: { status: 'COMPLETED', resultJson: JSON.parse(output) } })

          // (M6.5) 发送“结束”信号
          await kv.publish(kvChannel, { type: 'end', data: '[DONE]' })
        },
      }
    )

    // (M6.5) 将 LLM Token 实时转发到 KV Pub/Sub
    const reader = stream.getReader()
    const decoder = new TextDecoder()

    while (true) {
      const { done, value } = await reader.read()
      if (done) break

      const token = decoder.decode(value)
      // (M6.5) 握手核心：发布 Token
      await kv.publish(kvChannel, { type: 'token', data: token })
    }

    return NextResponse.json({ success: true, streaming: true })
  } catch (error) {
    // (M9) 更新 DB 状态 FAILED, (M2) 返还金币
    // (M6.5) 发布“错误”信号
    await kv.publish(kvChannel, {
      type: 'error',
      data: (error as Error).message,
    })
    return NextResponse.json(
      { error: (error as Error).message },
      { status: 500 }
    )
  } finally {
    if (lockKey) await releaseConcurrencyLock(modelId, lockKey)
    if (queueId) await decrementQueueCounter(queueId)
  }
}
export const POST = verifySignatureEdge(handler)
```

#### 任务 7：(AI IDE) DoD 测试脚本 (M6.4) - 【升级版】

- **目标**：验证 M6 的**所有核心基建**：背压、并发锁 和 QStash 生产者 均能协同工作。
- **文件**：`scripts/test-m6-queue.ts` (AI IDE 必须使用此版本)

<!-- end list -->

```typescript
// AI IDE: Use this FULL content for scripts/test-m6-queue.ts
import { getTaskRouting } from '@/lib/llm/task-router'
import { incrementQueueCounter } from '@/lib/kv/counter'
import { pushTaskToQueue, QStashPayload } from '@/lib/queue/producer'
import { kv } from '@/lib/kv/client'
import { TaskTemplateId } from '@/lib/prompts/types'
import { ModelId } from '@/lib/llm/providers'

// --- 配置 ---
const TEST_QUEUE_ID: QueueId = QueueId.FREE_BATCH // 我们测试“免费批处理队列”
const TEST_MODEL_ID: ModelId = ModelId.GLM_4_5_FLASH // 对应的模型
const TEST_TASK_ID: TaskTemplateId = 'resume_summary' // 对应的任务
const TEST_WORKER_NAME = 'batch-processor' // 对应的 Worker
const TEST_USER_ID = 'm6-test-user'

// M6.2 背压测试：设置一个极小的队列上限
const MAX_QUEUE_LENGTH = 2

// M6.1 并发锁测试：在 Worker 中设置并发为 1
// (注意：为使此测试生效，AI IDE 必须在 'app/api/worker/batch-processor/route.ts' 中
// 将 acquireConcurrencyLock(modelId, 5) 的 '5' 暂时硬编码为 '1')
const MAX_CONCURRENCY = 1
// ----------------

/**
 * M6.4 DoD 测试
 */
async function testM6_Infrastructure() {
  console.log('--- Starting M6 Infrastructure DoD Test ---')
  console.log(`[Config] Testing Queue: ${TEST_QUEUE_ID}`)
  console.log(
    `[Config] Testing Model: ${TEST_MODEL_ID} (Concurrency: ${MAX_CONCURRENCY})`
  )
  console.log(`[Config] Testing Backpressure: ${MAX_QUEUE_LENGTH}`)
  console.log('-------------------------------------------')

  // 0. 清理环境
  await kv.del(`lock:${TEST_MODEL_ID}`)
  await kv.del(`counter:${TEST_QUEUE_ID}`)
  console.log('KV locks and counters cleared.')

  // --- 1. 测试 M6.2 背压计数器 ---
  console.log(
    `\n[Test 1/3] Testing Backpressure (Limit: ${MAX_QUEUE_LENGTH})...`
  )

  let count = 0

  // 尝试推送 Task 1 (应成功)
  let check1 = await incrementQueueCounter(TEST_QUEUE_ID, MAX_QUEUE_LENGTH)
  count = check1.current
  console.log(`Push 1: Success=${check1.success}, Count=${check1.current}`)
  expect(check1.success).toBe(true)

  // 尝试推送 Task 2 (应成功)
  let check2 = await incrementQueueCounter(TEST_QUEUE_ID, MAX_QUEUE_LENGTH)
  count = check2.current
  console.log(`Push 2: Success=${check2.success}, Count=${check2.current}`)
  expect(check2.success).toBe(true)

  // 尝试推送 Task 3 (应失败，队列已满)
  let check3 = await incrementQueueCounter(TEST_QUEUE_ID, MAX_QUEUE_LENGTH)
  console.log(`Push 3: Success=${check3.success}, Count=${check3.current}`)
  expect(check3.success).toBe(false) // 关键：背压必须生效

  console.log('[Test 1/3] Backpressure test PASSED.')

  // --- 2. 测试 M6.3 QStash 生产者 ---
  console.log('\n[Test 2/3] Testing QStash Producer...')

  const payload: QStashPayload = {
    taskTemplateId: TEST_TASK_ID,
    modelId: TEST_MODEL_ID,
    locale: 'zh',
    context: {
      userId: TEST_USER_ID,
      taskId: 'm6-task-1', // Task 1
      queueId: TEST_QUEUE_ID,
      resume_text: 'M6 测试 1',
    },
  }

  // (M6.3) 推送 Task 1
  await pushTaskToQueue(TEST_WORKER_NAME, payload)
  console.log('Task 1 pushed to QStash.')

  // (M6.3) 推送 Task 2
  await pushTaskToQueue(TEST_WORKER_NAME, {
    ...payload,
    context: {
      ...payload.context,
      taskId: 'm6-task-2',
      resume_text: 'M6 测试 2',
    },
  })
  console.log('Task 2 pushed to QStash.')

  console.log('[Test 2/3] QStash producer test PASSED (2 tasks queued).')

  // --- 3. 验收 M6.1 (并发锁) & M6.7 (Worker) ---
  console.log('\n[Test 3/3] Awaiting Concurrency Lock (M6.1) Test...')
  console.log('---')
  console.log('**DoD 验收标准 (请在 Vercel 实时日志中观察)：**')
  console.log(`(前提: batch-processor worker 的并发锁已设为 1)`)
  console.log('1. [BatchWorker] 应被 QStash 触发 (Task 1)')
  console.log('2. [BatchWorker] 打印 "Lock acquired..." (Task 1)')
  console.log('3. (M4) LLM Service 应被调用 (Task 1)')
  console.log('4. [BatchWorker] 打印 "Task m6-task-1 completed."')
  console.log('5. [BatchWorker] 打印 "Lock released..."')
  console.log(
    '6. [BatchWorker] 打印 "Counter decremented..." (此时 KV 计数器 = 1)'
  )
  console.log('---')
  console.log('7. [BatchWorker] 应 *在此之后* 被 QStash 触发 (Task 2)')
  console.log('8. [BatchWorker] 打印 "Lock acquired..." (Task 2)')
  console.log('9. ... (Task 2 重复 3-5 步)')
  console.log(
    '10. [BatchWorker] 打印 "Counter decremented..." (此时 KV 计数器 = 0)'
  )
  console.log('\n**如果任务 2 在任务 1 释放锁之前启动，则 M6.1 失败！**')
  console.log('**如果 KV 计数器最终不为 0，则 M6.2 失败！**')
}

// 确保在 Vitest 环境或 tsx 脚本中运行
if (typeof describe === 'undefined') {
  testM6_Infrastructure().catch(console.error)
} else {
  describe('M6 Infrastructure Test', () => {
    it('should run the test script', async () => {
      // Vitest 环境下需要模拟 KV 和 QStash
      // 此处假设为脚本执行
      expect(true).toBe(true)
    })
  })
}
```

---

**M6 交付物：**
AI IDE（我）现在已经为你提供了 M6 里程碑的全部核心产物：

1.  **`lib/llm/task-router.ts`** (已更新)
2.  **`lib/kv/client.ts`** (新建)
3.  **`lib/kv/lock.ts`** (新建)
4.  **`lib/kv/counter.ts`** (新建)
5.  **`lib/queue/producer.ts`** (新建)
6.  **`app/api/sse-stream/route.ts`** (新建 - M6.5 握手路由)
7.  **`app/api/worker/batch-processor/route.ts`** (新建 - M6.6 骨架)
8.  **`app/api/worker/stream-processor/route.ts`** (新建 - M6.7 骨架)
9.  **`scripts/test-m6-queue.ts`** (新建 - DoD 测试脚本)

**项目 Owner 的 M6 DoD 验收：**

1.  AI IDE 提交 M6 的所有代码。
2.  你（Owner）**手动修改** `app/api/worker/batch-processor/route.ts` 中的 `acquireConcurrencyLock` 并发数，将其**暂时设为 1**。
3.  你（Owner）部署 `preview` 版本。
4.  你（Owner）在本地运行 `pnpm dlx tsx scripts/test-m6-queue.ts`。
5.  你（Owner）在 Vercel 实时日志中**观察**，确认日志**串行**打印，符合测试脚本（任务 7） 的验收标准。

# 4. 方案评审

## 理解与结论

- M6 的目标是为 LLM 推理建立可靠的异步调度基建：QStash 驱动任务投递与重试、Redis 提供并发控制与背压计数、SSE 将结果稳定地推送给前端，覆盖“流式”和“批处理”两类任务形态。
- 整体方案方向是可行的，且契合你的 Next.js 15 + Server Actions First + DAL 必须原则；但在具体实现细节上，仍有若干需要校准的点，以降低实现风险并确保与现有代码结构一致。

## 关键评审

- Upstash 通道选择
  - 建议将“实时推送（SSE 的服务端订阅）”从 Redis REST 的思路，切换为 Upstash Pub/Sub 产品（`@upstash/pubsub`），因为 Redis REST 客户端并不支持稳定的长连接式订阅；Pub/Sub 的 `subscribe()` 返回异步迭代器，适配 Edge Runtime 与 SSE 消息桥。
  - 保留 Redis 作为计数器与锁的后端（`INCR/DECR/SETNX/PEXPIRE`），形成“Pub/Sub 负责流、Redis 负责控”的职责分离。
- 频道命名一致性
  - 落地指南中 SSE 路由订阅的是 `stream:${userId}`，而 Worker 发布的是 `stream:${userId}:${serviceId}`。应统一到带 `serviceId` 的完整频道，以避免同一用户并发任务串流交叉污染。
- QStash API 使用细节
  - `publishJSON` 中是否支持 `queue` 字段需要核实当前 SDK 版本；若不支持队列概念，应通过“不同 Worker 路径 + 头信息/任务元数据”的方式区分通道，或改用 Topic/不同 URL。
  - 对于“任务依赖（dependsOn）”，不建议在 QStash 层强绑定（API 可能不稳定）；推荐通过 Worker 成功后显式触发下一任务（或写入 DB 状态，前置路由查询后再发布）。
- DAL 原则与事务
  - 落地示例中存在直接 `db.resume.update` 的写法；需全部改为 `lib/dal/*` 方法调用以遵循 DAL 必须原则。
  - 任务状态更新（如写入日志、扣配额、退款）涉及事务与幂等，建议 DAL 内部统一封装事务（Prisma `transaction`/`$transaction`），并记录幂等键（`taskId`, `refundId`）避免 QStash 重试造成重复。
- 并发锁与背压
  - 锁建议采用 `SETNX` + `PEXPIRE` 的轻量信号量（每资源一把），并明确最大持有时长和续租策略（流式任务最长，批处理较短）。避免使用集合 `SCARD` 走复杂集合语义。
  - 背压计数器应在 Worker“开始前 +1，成功/失败后 -1”，并考虑 QStash 重试导致的重复调用，通过幂等键阻止二次扣减。
- 流式与批处理分层
  - 流式任务建议对齐 “LangChainStream/Vercel AI SDK”，先将 token 侧输出统一（事件名、payload 结构、完成标记），Worker 仅负责发布消息，不做复杂组装。
  - 批处理任务统一返回结构化 JSON，SSE 路由可选择只做轻微包裹（如进度事件），不强行转成流式。
- 安全与边界
  - QStash Worker 路由必须首要校验 `verifySignatureEdge()`，并对 `userId` 进行鉴权（确保来自合法用户的触发）。SSE 路由也需鉴权（但仅允许订阅自己的频道）。
  - Redis/PubSub 通道命名使用不可枚举的、包含 `taskId` 的唯一键，防止越权订阅。
- 配置与现有目录对齐
  - 你现有代码里已有 `lib/redis/counter.ts` 与 `lib/concurrencyLock.ts`，建议复用并扩展，避免新增 `lib/kv/*` 造成风格不一致。此处待分析存量代码情况，如非必要，可删掉、彻底重构。
  - 认证模块统一使用你项目现有的 Neon Auth 包装（如 `lib/auth/wrapper.ts` / `lib/api/auth-wrapper.ts`），不要引入未存在的 `@job-assistant/authkit` 字样。
- 观测与日志
  - 强烈建议加“任务生命周期日志”与“消息投递日志”（添加 `taskId`, `serviceId`, `queue`, `userId` 等维度），塞到一个统一的日志 DAL，方便在 Vercel 日志和数据库中交叉排查。

## 补充与调整建议

- 事件规范
  - 统一 SSE 事件名：`start`、`chunk`、`end`、`error`、`progress`；统一字段结构：`{ taskId, serviceId, stage, data, ts }`。
  - 流式任务在 `end` 事件时发布最终聚合“完成”标识，前端据此结束渲染与关闭连接。
- 幂等与重试策略
  - Worker 输入携带 `taskId`，DAL 层以 `taskId` 做写操作幂等约束（状态更新、配额扣减）。
  - 退款仅在“明确失败且未退款”的状态触发，一次性事务，失败重试由 QStash 兜底，但 DAL 端检查“已完成/已退款”阻止重复。
- 资源级限流
  - 除用户级并发外，对模型/供应商（如 DeepSeek/GLM）做全局资源 ID 的并发上限（比如 5），避免击穿单一后端。
- 前后端联调
  - 先用“模拟模型”的流式/批处理假数据跑通从 Producer → Worker → PubSub → SSE → 前端 EventSource 的闭环，再切换真实模型。

# 5. 执行规划

- 01 路由与目录对齐
  - 创建/调整：`lib/queue/producer.ts`（QStash 发布）、`app/api/worker/stream/[serviceId]/route.ts`、`app/api/worker/batch/[serviceId]/route.ts`、`app/api/sse-stream/route.ts`
  - 统一认证包装与 Edge 运行时声明，Worker 路由启用签名校验。
- 02 计数器与并发锁
  - 扩展 `lib/redis/counter.ts`：`incr`, `decr`, `get`, `ttl`（含过期）
  - 扩展/复用 `lib/concurrencyLock.ts`：`acquireLock(resourceId, ttlMs)`, `releaseLock(resourceId)`
- 03 Pub/Sub 客户端
  - 新增 `lib/pubsub/client.ts` 使用 `@upstash/pubsub`，封装 `publish(channel, event)`, `subscribe(channel)`；频道命名：`stream:${userId}:${serviceId}:${taskId}`
- 04 QStash Producer
  - `pushTaskToQueue()` 支持两类任务：`stream`、`batch`；根据 `serviceId/queueId` 决定目标 Worker 路径；打上元数据（`taskId`, `userId`, `locale`, `quotaMode`）
  - 不依赖 `queue` 字段，如确需逻辑队列，通过 URL 选择或自定义 Header 实现
- 05 Worker 路由实现
  - Stream Worker：校验并发锁与配额 → 递增背压 → 调用 `runStreamingLlmTask()` → 将 chunk 通过 Pub/Sub 发布 → 结束事件 → 释放锁 & 背压 -1 → DAL 写入日志与成功状态
  - Batch Worker：同上流程，但产出一次性 JSON；失败路径做退款（幂等检查）
- 06 SSE 路由实现
  - 鉴权通过后，`EventSource` 推送，内部用 `for await` 订阅 Pub/Sub 的频道迭代消息并转发
  - 事件格式统一（见“事件规范”），最终 `end` 关闭 SSE
- 07 DAL 封装
  - 新增/扩展：`lib/dal/tasks.ts`（任务状态与日志）、`lib/dal/quotas.ts`（扣减/退款，幂等事务）
  - 严禁在 Worker/SSE 中直接用 `PrismaClient`
- 08 环境与配置
  - `.env.local`：`QSTASH_TOKEN`, `QSTASH_CURRENT_SIGNING_KEY`, `QSTASH_NEXT_SIGNING_KEY`, `UPSTASH_PUBSUB_URL/TOKEN`, `UPSTASH_REDIS_REST_URL/TOKEN`
  - 仅在服务端读取非 `NEXT_PUBLIC_` 变量
- 09 联调与测试
  - 脚本：`scripts/test-m6-queue.ts` 生成模拟任务；前端用简单页面 `EventSource` 订阅 SSE，展示 token 流或批处理 JSON
  - 对“重试/幂等/锁续租/背压上下线”做专项测试
- 10 观测与告警
  - 统一日志埋点；关键指标：发布成功率、Worker 成功率、平均延迟、背压峰值、锁等待时间；失败率突破阈值触发告警（可先日志级别提升）

**依赖与环境约束**

- Next.js 15 App Router，默认 RSC；SSE 路由为 Edge Runtime；Worker 路由建议 Edge，但若模型 SDK 仅 Node 支持，则切到 Node（并在发布时区分）。
- DAL 为唯一数据入口；所有队列和日志写操作必须通过 DAL。
- shadcn/ui 与 Tailwind 不涉及本轮改动；避免 UI 改动（无预览动作需求）。

**验收标准（DoD）**

- 能从前端调用 Server Action → 后端立即返回 `taskId`，随后前端通过 SSE 收到有序的 `start/chunk/progress/end` 事件。
- 并发控制与背压计数在单/多用户下稳定生效，无越权订阅和流串扰。
- QStash 重试不导致重复扣费/重复写状态；DAL 幂等生效。
- 失败路径自动退款且日志可追溯到具体 `taskId`、`serviceId`、`queueId`。
- 无直接 `PrismaClient` 使用，全部走 DAL；环境变量使用规范。

# 6. 处理流程（by 场景）

以下讲解覆盖三条闭环主线（职位匹配 → 简历定制 → 面试准备），按“生产者 → Worker → 事件/SSE → 观测”拆解：

补充说明（Walkthrough：三种场景）

- 流程总览：前端触发 `pushTask` → 入队（限流/幂等校验与审计）→ Worker 执行（并发锁与背压）→ 事件发布到 Pub/Sub + 写入 Streams → 前端 SSE 实时/批量展示 → 用量/费用/耗时统一记录。
- 职位匹配（Stream）：`templateId=job_match`，有配额选 `deepseek-reasoner`，否则 `glm-4.5-flash`；逐块 `token` 输出，末尾 `done` 汇总。
- 简历定制（Batch）：`templateId=detailed_resume_summary` 或 `resume_customize`；有配额选 `deepseek-reasoner/deepseek-chat`，否则 `glm-4.5-flash`；一次性结构化 JSON 于 `done` 事件返回。
- 面试准备（Stream）：`templateId=interview_prep`；有配额选 `deepseek-chat`，否则 `glm-4.5-flash`；连续 `token` 输出并在 `done` 收尾。
- 视觉分支（Batch）：若变量含 `image/jobImage`，按 `getJobVisionTaskRouting` 走 `glm-4.1v-thinking-flash`，批处理完成。

- 统一通道与计数器
  - `channel`: `cs:events:{userId}:{serviceId}:{taskId}`（Pub/Sub 推送，同时写入 Redis Streams 作为 SSE 缓冲）
  - `counterKey`: `bp:{userId}:{serviceId}`（背压/并发计数键）

本地测试快速指南（简版）

- 设置 `.env.local`：`NEXT_PUBLIC_APP_BASE_URL`（指向 ngrok 域名）、`QSTASH_*`、`UPSTASH_*`。
- 运行 `npm run dev`，打开 `/(dev)/sse` 页面；在表单选择 `templateId`（`job_match`/`detailed_resume_summary`/`interview_prep`），提交后在 SSE 区域观察事件序列。
- 期望：流式场景看到连续 `token` + `done`；批处理场景看到一次性 `done`（含结构化 JSON）。如无事件，检查回调域名、签名与 Redis 连接。
- 生产者 `pushTask`
  - 限流：`checkRateLimit('pushTask', userId, shouldUseFreeQueue)`；超限审计 `rate_limited`，返回 `{ rateLimited, retryAfter }`，不入队
  - 幂等：按模板映射 `match/customize/interview` 生成幂等键；命中审计 `idempotent_replay`，返回 `{ replay, idemKey }`，不入队
  - 审计：成功入队审计 `enqueue`（含 `messageId/url`）
  - 目标 URL：`${NEXT_PUBLIC_APP_BASE_URL || 'http://localhost:3000'}/api/worker/{stream|batch}/{serviceId}`
- Worker（Stream/Batch 通用）
  - 并发/背压：`enterGuards`（锁 + `bumpPending`）→ 处理 → `exitGuards`（`decPending` + 释放锁）
  - 路由：`getTaskRouting(templateId, hasQuota)` 或 `getJobVisionTaskRouting(hasQuota)`（按付费/免费与是否图片，决定 `modelId/queueId/isStream`）
  - 事件：`publishEvent` 推送 `start/token/progress/done/error`（附审计 `event_publish`），并写 Streams 缓冲
  - 用量日志：`createLlmUsageLogDetailed`（模型、tokens、时延、费用、成功标记）
- SSE 路由
  - 读取：`/api/sse-stream` 用共享客户端 `getRedis().xrange(streamKey, start, '+')` 顺序读取 Streams；维持心跳并在连接中止时清理

场景一：职位匹配（Job Match，Stream）

- 触发：Server Action 或 dev 页面调用 `pushTask({ kind:'stream', templateId:'job_match', ... })`
- 生产者：限流 + 幂等（step=`match`）通过后入队；审计入队
- Worker：`getTaskRouting('job_match', hasQuota)` → 付费优先 `deepseek-reasoner`，免费 `glm-4.5-flash`；`isStream=true`
- 阶段划分（跨步骤与可扩展）：
  - `stage=ingest`：JD 预处理（OCR/去噪/统一字段），产出结构化要点（角色、职责、技能）
  - `stage=compare`：对简历关键段落进行语义比对，生成匹配/不匹配点及证据段落引用
  - `stage=score`：综合评分与建议（硬技能/软技能/经验匹配度），给出改写提示清单
  - `stage=deliver`：整理为用户可读的增量输出与最终总结（用于 SSE 展示与可复制内容）
- 事件类型（SSE 与审计）：`start`（含路由决策）→ 多次 `token`（增量）/可选 `stage_progress`（阶段里程碑）→ `done`（含汇总文本与用量）→ `error`（异常时）
- 执行：LCEL `prompt | model` 流式产出；阶段切换时附带 `stage_progress` 事件；`token` 逐块发布，`done` 收尾（携带汇总文本/用量）
- 观测：`event_publish` 审计 + Redis Streams 缓冲；`createLlmUsageLogDetailed` 记录 tokens/费用/时延；SSE 页面可见增量输出与阶段提示
- 测试可操作性：
  - 建议用 dev 页触发 `templateId=job_match`，观察 `start → token → stage_progress → done` 事件序列；
  - 在低配额/免费模式下验证 `glm-4.5-flash` 的回退；
  - 用极端 JD（长文本/混合语言/弱结构）验证 `ingest` 阶段的稳健性；
  - 通过 `/api/sse-stream?userId=...&serviceId=...&taskId=...` 检查断线重连时 Streams 缓冲是否补齐事件。

场景二：简历定制（Resume Customize，Batch）

- 触发：`pushTask({ kind:'batch', templateId:'resume_customize' | 'detailed_resume_summary', ... })`
- 生产者：限流 + 幂等（step=`customize`）通过后入队；审计入队
- Worker：`getTaskRouting('detailed_resume_summary', hasQuota)` → 付费 `deepseek-reasoner`，免费 `glm-4.5-flash`；`isStream=false`
- 执行：`runStructuredLlmTask(modelId, templateId, locale, variables, { userId, serviceId })` 返回结构化 JSON；发布 `done`
- 观测：`event_publish` 审计 + Streams 缓冲；`createLlmUsageLogDetailed` 记录结构化用量与成功标记；SSE 页面看到一次性 JSON 结果

场景三：面试准备（Interview Prep，Stream）

- 触发：`pushTask({ kind:'stream', templateId:'interview_prep', ... })`
- 生产者：限流 + 幂等（step=`interview`）通过后入队；审计入队
- Worker：`getTaskRouting('interview_prep', hasQuota)` → 付费优先 `deepseek-chat`，免费 `glm-4.5-flash`；`isStream=true`
- 执行：问答/要点生成流式输出；逐块 `token` + 收尾 `done`
- 观测：同 Job Match（审计/缓冲/用量日志）；SSE 页面增量展示要点

补充：视觉分支（Vision）

- 若 `variables` 含 `image`/`jobImage`，切换至 `getJobVisionTaskRouting(hasQuota)` → `glm-4.1v-thinking-flash`，`isStream=false`
- 当前 dev 页面未开放图片上传字段，留待后续迭代用脚本或表单扩展模拟

# 7. 本地开发环境测试操作指南

目标：在本地以 ngrok 暴露服务情况下，走通“职位匹配（流式）/简历定制（批处理）/面试准备（流式）”三条主线并观察 SSE。

环境准备

- `.env.local` 需配置：
  - `NEXT_PUBLIC_APP_BASE_URL`：指向 ngrok 域名（例如 `https://xxxx.ngrok.io`），用于 QStash 回调本地
  - `QSTASH_TOKEN`, `QSTASH_CURRENT_SIGNING_KEY`, `QSTASH_NEXT_SIGNING_KEY`
  - `UPSTASH_REDIS_REST_URL`, `UPSTASH_REDIS_REST_TOKEN`
- 说明：本项目仅使用 Upstash Redis 的 REST API 与 Redis 命令（Publish/Streams/XLEN/XADD/XRANGE 等）来实现事件分发与缓冲，不依赖“Upstash Pub/Sub”独立产品。现有代码已统一通过 `lib/redis/client.ts` 和 `lib/worker/common.ts` 进行发布与缓冲，无需额外的 Pub/Sub URL/TOKEN。

启动与页面

- 启动开发：`npm run dev`
- 打开：`/[locale]/sse` 页面（例如 `https://xxxx.ngrok.io/zh/sse` 或本地 `http://localhost:3000/zh/sse`）
- 该页面包含一个表单（触发 `pushTask`）与一个 SSE 查看器（订阅 `/api/sse-stream?userId=...&serviceId=...&taskId=...`）

三场景操作步骤

1. 职位匹配（Stream）
   - 在表单填写：`userId/serviceId/taskId`（默认即可），`templateId=job_match`
   - 点击提交后观察 SSE：先见连接/心跳，随后持续 `token`，最终 `done`
2. 简历定制（Batch）
   - 在表单填写：`templateId=detailed_resume_summary`（或 `resume_customize`，若已提供对应模板）
   - 点击提交后观察 SSE：等待一段时间，收到一次性 `done`（包含结构化 JSON）
3. 面试准备（Stream）
   - 在表单填写：`templateId=interview_prep`
   - 提交后观察 SSE：与职位匹配类似，连续 `token`，最后 `done`

关键验证点

- 幂等性：使用相同 `userId/serviceId/taskId/templateId/variables` 重复提交，生产者可能直接返回 `{ replay, idemKey }` 而不入队；SSE 无新增事件
- 限流：短时间连续多次提交，观察生产者返回 `{ rateLimited, retryAfter }` 与审计日志 `rate_limited`；SSE 无新增事件（除非缓冲中的历史事件）
- 背压/并发：在多个窗口或不同 `serviceId` 并发提交流式任务，确保事件按 `taskId` 不串扰；出错时查看 Worker 的 500/错误事件

故障排查

- QStash 回调失败：确认 `NEXT_PUBLIC_APP_BASE_URL` 指向 ngrok 域名，且 ngrok 允许外部访问；查看 `/api/worker/...` 返回码与签名校验
- Upstash 连接问题：核对 `UPSTASH_*` 变量拼写与值；在日志中检查 `getRedis().xrange` 与 `publishEvent` 是否报错
- SSE 不出数据：检查 Redis Streams 是否有写入（`XADD`），以及订阅参数 `userId/serviceId/taskId` 与 Worker 推送一致

可选扩展（Vision）

- 若需模拟视觉任务：在请求变量中加入 `image`/`jobImage`（需要扩展 dev 表单或编写脚本），将走 `getJobVisionTaskRouting` 并以批处理完成

# 8. Redis 读写平衡策略

目标：在保证事件顺序与体验的前提下，降低 Upstash 读写频率与总体开销。策略覆盖两端：事件生产端（发布/入流）与 SSE 消费端（读取/推送）。

生产端（publishEvent）

- Token 合并：对 `type=token` 的事件进行“时间窗口 + 长度阈值”双触发合并。
  - 时间窗口：`STREAM_FLUSH_INTERVAL_MS`（默认 400ms）。窗口到达即 flush 合并缓存。
  - 长度阈值：`STREAM_FLUSH_SIZE`（默认 8）。累计达到阈值立即 flush（不等待窗口）。
- 合并事件格式：写入 `token_batch`，含 `text`（拼接后的字符串）、`count`（合并 token 数）、`startedAt/endedAt`（窗口时间）。
- 顺序保证：在写入任何非 token 事件（如 `start/done/error`）前，先强制 flush 当前通道的 token 缓存，保证事件序列有序。
- 双通道写：仍向 Redis Pub/Sub 发布，同时同步 `XADD` 到 Redis Streams（作为 SSE 的后备缓冲）。这两个写操作都使用合并后的 payload，降低总写次数。
- 调试门控：生产环境抑制 `debug` 事件（不发布、不入流）。非生产环境保留 `LLM_DEBUG` 下的关键调试事件采样。

消费端（SSE 路由 `app/api/sse-stream/route.ts`）

- 从最新事件读取：`fromLatest=1` 时，起始 `lastId` 取 `'$'`，仅拉取后续事件，避免历史积压导致的冷启动读放大。
- 读取与心跳：每次拉取完将事件格式化为 SSE 字符串后推送给客户端；收到终止事件（`done` 或流式阶段的致命 `error`）后主动关闭连接以减少轮询。
- 等效阻塞（自适应长轮询）：根据最近是否有新事件动态调整下一次读取的延迟。
  - 基准延迟对齐：`base = STREAM_FLUSH_INTERVAL_MS`（最少 400ms），与生产端合并窗口保持一致。
  - 自适应回退：连续空读次数 `n` 增加时，延迟按 `min(8000, base * 2^n)` 增长，最高回退到 8 秒，等效于 BLOCK 行为但无需依赖 Upstash 的阻塞读。

前端 Viewer（`SseStreamViewer.tsx`）

- 兼容展示：新增对 `token_batch` 的友好文案支持，显示 `+{chars} chars · {count} tokens`，与单个 `token` 事件一致的视觉语义。
- 终止关闭：收到 `done` 或特定 `error` 时主动关闭 SSE 连接，减少 Redis 空读。

环境变量

- `STREAM_FLUSH_INTERVAL_MS`：时间窗口，默认 `400`。
- `STREAM_FLUSH_SIZE`：长度阈值，默认 `8`。
- `LLM_DEBUG`：调试事件采样，仅在非生产环境下使用；生产环境抑制 `debug` 事件的发布与入流。

效果与权衡

- 写入次数减少：高频 token 合并可显著降低 `publish` 与 `XADD` 的调用次数。
- 读取次数降低：路由读延迟对齐并回退，空闲期读频率显著下降。
- 顺序与一致性：非 token 事件发生前强制 flush，保证事件序列的时间与语义一致。
- 观测与审计：保留 `start/done/error` 的审计；跳过 `token/token_batch/debug` 的审计以降低日志噪声。

# 9. Debug Notes

可复用的命令速记

- 直接 DeepSeek ping： npx tsx scripts/ping-deepseek.ts
- LangChain + ChatDeepSeek ping： npx tsx scripts/ping-deepseek-langchain.ts
- LangChain + ChatOpenAI（DeepSeek baseURL）对比： npx tsx scripts/ping-deepseek-openai.ts
- 带真实数据跑 job_match ： npx tsx scripts/debug-llm-call.ts --modelId deepseek-chat --templateId job_match --locale zh --vars '{}' --timeout 40000 --mode invoke --resumeFile /Users/edisonmbli/Projects/CareerShaper/docs/llm_testing/resume.txt --jobFile /Users/edisonmbli/Projects/CareerShaper/docs/llm_testing/jd.txt

# 10. 如何保障经 Server Actions 发起的任务能及时地终止不必要的 Redis 读/写操作？

本节梳理生产环境下（Server Actions → QStash → Worker → Streams/SSE）的“任务生命周期与 Redis I/O 收敛”原理，并给出最佳实践与落地清单，确保任务在成功/失败后能够即时、可靠地关闭与 Redis 的读/写通路，避免产生不必要的访问量与成本。

## 原理概述（当前实现）

- 事件发布与缓冲：`lib/worker/common.ts` 的 `publishEvent(channel, event)` 将关键事件写入 Redis Pub/Sub，同时将事件镜像到 Redis Streams（键：`cs:events:{userId}:{serviceId}:{taskId}:stream`），便于 `/api/sse-stream` 进行轮询式桥接。
- Token 合并与冲洗：对 `type=token` 的事件采用“时间窗口 + 长度阈值”的合并策略；对非 token 事件（如 `done`/`error`），在写入前会执行一次 `flush(channel)` 冲洗，确保顺序正确、定时器取消、合并缓存清空。
- 终止事件与关闭：`app/api/worker/stream/[service]/route.ts` 在流式推理结束时发布 `type=done` 或在异常时发布 `type=error`。`/api/sse-stream` 在读取到终止事件后主动 `controller.close()`，并停止后续的轮询读取；客户端断开（`req.signal.abort`）也会触发关闭与清理。
- 守卫收尾：Worker 在 `finally` 中调用 `exitGuards(userId, 'stream', counterKey)`，下调并发计数、释放锁，避免进一步的 Redis 写入压力。

上述协作保证了“终止事件触发 → SSE 关闭 → Token 合并定时器被取消 → 并发计数下调”的闭环，从而在连接关闭后不再继续读/写 Redis。

## 职责边界与最佳实践

1. Server Actions（入口与调度）

- 只负责参数校验、配额/并发守卫、生成 `taskId` 和 `channel`，并通过 QStash 投递任务；不直接维护 SSE 连接，也不直接操作 pub/sub 写入。
- 建议携带“业务截止”信息（如 `deadlineMs`、`traceId`），用于 Worker 超时保护与审计。
- 不需要主动“杀掉” Redis 通道；生命周期的终止由 Worker 发出终止事件并由 SSE 桥接关闭来完成。

2. Worker（执行与事件源）

- 流式任务：在产生 `token` 时仅调用 `publishEvent(channel, { type:'token', ... })`；结束时务必发送**唯一且紧随其后的**终止事件（`done` 或 `error`）。
- 异常处理：`catch` 分支发布 `type=error`，其属于非 token 事件，会触发 `flush(channel)`，从而清空合并缓存、取消定时器，避免尾部写入。
- 收尾：`finally { await exitGuards(...) }` 保证并发计数及时回落、锁被释放。
- 调试事件抑制：生产环境下 `publishEvent` 会直接跳过 `type=debug`（不发布、不入流），避免无效 Redis 写入。

3. SSE 桥接（读取与关闭）

- 终止事件关闭：`/api/sse-stream` 在读取到 `type=done` 或特定 `type=error` 后立即 `controller.close()` 并停止轮询，见代码：
  ```ts
  // app/api/sse-stream/route.ts（节选）
  const isTerminal =
    data?.type === 'done' ||
    (data?.type === 'error' &&
      ['invoke_or_stream', 'invoke', 'guards'].includes(String(data?.stage)))
  if (isTerminal) {
    closed = true
    if (timer) clearTimeout(timer)
    controller.close()
    return
  }
  ```
- 客户端断开关闭：`req.signal.addEventListener('abort', ...)` 上报后也会触发关闭逻辑，及时停止后续读取。

## 生产级 Best Practices（建议在 M8/M9 中遵循）

- 单一终止事件约束：每个任务在 Worker 侧必须保证“恰好一次”的 `done` 或 `error`，并在其之前冲洗 token 缓存。该约束是关闭读/写的根源信号。
- finally 清理必不可少：所有流式/批处理 Worker 在 `finally` 中执行 `exitGuards`，确保并发计数与锁状态恢复；避免遗留导致后续任务误判队列压力。
- TTL 与修剪：对 `cs:events:{...}:stream` 这种临时缓冲键设置短 TTL（2–5 分钟）或采用 `XTRIM MAXLEN` 控制长度，降低长尾成本（可以在 `done/error` 后通过 Upstash Pipeline 追加 `EXPIRE`/`XTRIM` 指令）。已把“TTL/修剪”的优化统一抽象并落到了 publishEvent 的终止事件分支（ done / error ）。这样 M8、M9 的服务只需调用 publishEvent ，无需在各自业务里重复编写清理逻辑。
- 控制通道（可选）：在某些需要显式指令的前端（非 SSE 桥接）使用 `cs:control:{...}` 频道发布 `{ type:'control', op:'close' }`，确保客户端自闭环；当前 SSE 桥接已在终止事件自动关闭，一般无需此步。
- 严格抑制调试写入：生产环境不发布 `type=debug`；开发环境的 Redis 采样器只在有变化时输出，减少噪音与不必要请求。
- 超时与熔断：使用 `ENV.WORKER_TIMEOUT_MS` 设置上限；超时视为失败并发布 `error` 终止事件，从而触发 SSE 关闭与守卫收尾。
- 配额/并发守卫前置：Server Actions 侧严格执行配额与并发守卫，避免不必要的任务入列，从源头降低 Redis 压力。

## 最小落地清单（代码位置与动作）

- `app/api/worker/stream/[service]/route.ts`
  - 结束时发布 `type=done`，异常时发布 `type=error`；保持 `try/catch/finally` 结构，`finally` 中执行 `exitGuards`。
- `lib/worker/common.ts`
  - 保持“非 token 事件前先 `flush(channel)`”的顺序保障；生产环境抑制 `type=debug` 发布与入流。
  - 可选：在检测到 `done/error` 后对 `streamKey` 执行 `EXPIRE` 或 `XTRIM`（建议 2–5 分钟 TTL）。
- `app/api/sse-stream/route.ts`
  - 在终止事件或 `req.signal.abort` 时关闭 SSE；保持自适应轮询（空闲时退避、活跃时低延迟）。
- Server Actions（各 `*.actions.ts`）
  - 参数校验、配额与并发守卫、生成 `taskId/channel`、向 QStash 投递；不直接操作 Redis 关闭，生命周期终止由 Worker/SSE 完成。

## 需要避免的误区

- 不要在 Server Actions 中直接“强杀” Redis 通道或连接。正确做法是让 Worker 发出终止事件并让 SSE 桥接完成关闭与清理，从而保持职责清晰、实现简单可靠。
- 不要发布多个终止事件或在终止后继续写入 token；这会导致 SSE 端无法合理关闭或出现残留定时器。

## 参考代码（节选）

```ts
// Worker（流式）：结束事件 + finally 守卫
let full = ''
for await (const chunk of stream) {
  const text = (chunk as any)?.content ?? (chunk as any)?.text ?? ''
  if (text)
    await publishEvent(channel, {
      type: 'token',
      text,
      stage: 'stream',
      taskId,
    })
}
await publishEvent(channel, {
  type: 'done',
  text: full,
  stage: 'finalize',
  taskId,
})
// finally
await exitGuards(userId, 'stream', counterKey)

// SSE：收到终止事件立即关闭
if (data?.type === 'done' || (data?.type === 'error' && shouldClose(data))) {
  closed = true
  if (timer) clearTimeout(timer)
  controller.close()
}
```

以上实践在当前代码结构下已基本具备：终止事件触发、SSE 关闭、自适应轮询停止、token 合并定时器被取消、并发计数回落。M8/M9 只需在需要时补充短 TTL/修剪即可达到更好的成本控制与稳定性。

# 11. 开发者速记 notes

## **整体概览**

- 核心链路围绕“生产者 → QStash → Worker → 事件发布（Pub/Sub + Streams）→ SSE 展示 → 审计与用量记录”，对流式与批处理两类任务做了统一的治理。
- 本轮关键增强集中在事件发布与资源清理：终止事件统一追加 `TTL`/`XTRIM`，流式 `token` 合并批量发布，生产环境抑制 `debug` 事件，SSE 桥接自适应轮询并在终止后主动关闭。
- 并发控制与背压通过 Redis 锁与计数器实现，Producer 侧加入限流与幂等防抖，Worker 侧加入签名校验、配额驱动的模型路由与“返回 200 防止重复重试”的明确策略。
- 这些协同支持文档 6 节（L989-999）中的四个场景：职位匹配（Stream）、简历定制（Batch）、面试准备（Stream）、视觉分支（Batch）。

## **Queue（QStash 客户端）**

- 文件 `lib/queue/qstash.ts` 提供 `getQStash()` 单例，基于 `ENV.QSTASH_TOKEN` 初始化 Upstash QStash 客户端。
- 只负责发消息，不参与业务逻辑；后续所有任务入队都通过该单例，确保连接与重试策略一致。

## **Producer（入队与前置治理）**

- 文件 `lib/queue/producer.ts` 的 `pushTask(params)` 是标准入队入口，构建目标 URL：`/api/worker/{stream|batch}/{serviceId}`。
- 前置校验协同：
  - 配额驱动限流：`checkQuotaForService` 决定是否走免费队列，再由 `checkRateLimit('pushTask', userId, shouldUseFreeQueue)` 执行速率限制。
  - 幂等防抖：将部分 `templateId` 映射到幂等步骤（`match/customize/interview`），在 TTL 窗内重复请求直接返回 `replay`，避免重复入队与重复计费。
  - 成功入队或提前返回均做 `auditUserAction`，便于审计与追踪。
- 返回值携带 `messageId/url/retryAfter/idemKey` 等元数据，前端可用其判断是否需要退避或展示提示。

## **Worker Common（统一工具与守卫）**

- 文件 `lib/worker/common.ts` 提供统一能力：
  - 请求体验证：`parseWorkerBody` 使用 Zod 校验任务体（包含 `taskId/userId/serviceId/templateId/variables`）。
  - 并发与背压：`enterGuards` 先拿并发锁（`acquireLock`），再 bump 计数器（`bumpPending`），失败则立即释放锁并返回 429，且响应 `Retry-After`。
  - 退出清理：`exitGuards` 在 `finally` 中执行 `decPending` 与 `releaseLock`，确保异常路径也能释放资源。
  - 事件发布：`publishEvent(channel, event)` 统一发布到 Pub/Sub，并同步写入 Redis Streams；关键增强：
    - 生产环境抑制 `debug` 事件发布与入流，降低不必要 I/O。
    - 针对 `token` 事件做时间窗口与长度阈值合并（`STREAM_FLUSH_INTERVAL_MS`、`STREAM_FLUSH_SIZE`），先合并后统一 `publish`/`XADD`，明显降低高频写入。
    - 在非 `token` 事件前强制 `flush`，保证事件顺序。
    - 对终止事件（`done/error`）在同一 Upstash `/pipeline` 中追加 `EXPIRE`（`STREAM_TTL_SECONDS`）与近似修剪 `XTRIM MAXLEN ~`（`STREAM_TRIM_MAXLEN`），减少长尾存储成本并加速资源回收。
    - 仅对 `start/done/error` 做审计（事件与链路元信息），兼顾可观测性与低噪音。

## **Worker（Batch 与 Stream 处理）**

- 文件 `app/api/worker/batch/[service]/route.ts`：
  - 通过 `verifySignatureAppRouter` 验签，确保入口只来自 QStash。
  - 先发布 `start` 事件（含 `timeoutSec/requestId/traceId`），再调用 `enterGuards`。
  - 路由模型：如有图片变量走 `getJobVisionTaskRouting`（视觉模型 `glm-4.1v-thinking-flash`），否则走 `getTaskRouting`（配额决定 `deepseek-*` 与 `glm-*`）。
  - 调用 `runStructuredLlmTask` 一次性生成结构化结果，于 `done` 事件返回；随后记录用量与费用（`createLlmUsageLogDetailed`）。
  - 异常统一发布 `error` 并在 `finally` 执行 `exitGuards`；对守卫失败返回 200，避免 QStash 重复投递。
- 文件 `app/api/worker/stream/[service]/route.ts`：
  - 同样验签与守卫流程，先发 `start`。
  - 在 `ENV.LLM_DEBUG` 时发布一次上下文 `debug` 事件（生产环境会被抑制）。
  - 使用 LCEL：`ChatPromptTemplate.pipe(getModel(...))` 流式生成；对每个 chunk 发布 `token` 事件（走合并策略），结束后发 `done`。
  - 尽力而为地采集 `tokenUsage`（不同模型字段名不一致时做兼容），记录用量与费用。
  - 异常路径发布 `error` 并 `exitGuards`，对早期 `invoke` 错误有单独的 `debug` 采样（生产抑制）。

## **SSE 桥接（轮询 Streams 并自清理）**

- 文件 `app/api/sse-stream/route.ts`：
  - `dynamic = 'force-dynamic'`；从 `channel:stream` 轮询读取，使用 `xrange` 的排他起始 `(${lastId}` 避免重复。
  - 支持 `fromLatest=1` 首次连接快进到最新一条（通过 Upstash `/pipeline` 的 `XREVRANGE COUNT 1`）。
  - 自适应轮询回退：空闲次数越高，`tick` 间隔指数退避（以 `STREAM_FLUSH_INTERVAL_MS` 为基线），降低空读成本。
  - 收到终止事件（`done` 或特定阶段 `error`）立即关闭连接与定时器，避免后续无效读取。
  - 连接 `abort` 时清理心跳与轮询，确保服务器侧没有悬挂处理。
- 文件 `lib/pubsub/channels.ts`：
  - 标准化频道命名：`cs:events:{userId}:{serviceId}:{taskId}` 与 `:stream` 后缀键；保证跨模块一致。
  - 预留控制通道 `cs:control:*`（未来用于非 SSE 客户端或外部控制）。

## **Concurrency Control（锁与背压）**

- 文件 `lib/concurrencyLock.ts`：
  - 基于 Upstash Redis `SET NX EX` 实现分布式锁；当 Redis 不可用时优雅降级为内存锁，保证本地开发可运行。
  - `releaseLock` 优先走 Redis，再回退内存。
- 文件 `lib/redis/counter.ts`（被 `enterGuards/exitGuards` 调用）：
  - `bumpPending(counterKey, ttl, maxSize)` 控制“队列内并发进行中”计数与上限（背压）；失败返回建议退避时间。
  - `decPending(counterKey)` 在 `finally` 扣减，避免泄漏。

## **场景对照（支持文档 L989-999 流程）**

- 职位匹配（Stream，`templateId=job_match`）：
  - Producer 入队 `kind=stream` → Worker（Stream）按配额选择 `deepseek-reasoner` 或 `glm-4.5-flash` → 持续发布 `token`（合并）与最终 `done` → SSE 按序展示，终止后关闭连接。
- 简历定制（Batch，`templateId=detailed_resume_summary/resume_customize`）：
  - Producer 入队 `kind=batch` → Worker（Batch）按配额选择 `deepseek-*` 或 `glm-4.5-flash` → 一次性 `done` 携结构化 JSON → SSE 读到终止事件后关闭。
- 面试准备（Stream，`templateId=interview_prep`）：
  - Producer 入队 `kind=stream` → Worker（Stream）按配额选择 `deepseek-chat` 或 `glm-4.5-flash` → 连续 `token` 与 `done` 收尾 → SSE 展示并在终止后清理。
- 视觉分支（Batch，含 `image/jobImage`）：
  - `hasImage(variables)` 命中 → Worker（Batch）走 `getJobVisionTaskRouting` → 使用 `glm-4.1v-thinking-flash` → 完成后 `done` 返回结构化结果并关闭。

## **治理与优化要点（本轮新增/调整）**

- 终止事件统一追加 Upstash Pipeline 的 `EXPIRE` 与 `XTRIM MAXLEN ~`，由 `ENV.STREAM_TTL_SECONDS` 与 `ENV.STREAM_TRIM_MAXLEN` 控制，极大降低长尾存储与后续冗余读取。
- 流式 `token` 合并批次发布，时间与长度双阈互补，事件顺序由“非 token 前强制 flush”保障。
- 生产环境抑制 `debug` 事件写入，观测转移到少数关键节点（`start/done/error`）与审计日志。
- SSE 自适应退避与终止后主动关闭，避免持续轮询空流；首次连接可快进到最新，减少重放。

## **失败处理与幂等/重试策略**

- 守卫失败（锁或背压）与 Provider 未配置一律返回 200 并发布 `error`，阻止 QStash 重复投递。
- Producer 幂等：命中重复请求直接返回 `replay`，审计记录原因与 `idemKey`。
- Worker `try/catch/finally` 全覆盖，异常路径确保 `exitGuards` 执行，避免并发锁与计数器泄漏。

## **关键配置项（环境变量）**

- `QSTASH_TOKEN/QSTASH_CURRENT_SIGNING_KEY/QSTASH_NEXT_SIGNING_KEY`：队列与验签。
- `WORKER_TIMEOUT_MS/CONCURRENCY_LOCK_TIMEOUT_MS/QUEUE_MAX_SIZE`：超时、锁 TTL 与背压上限。
- `UPSTASH_REDIS_REST_URL/UPSTASH_REDIS_REST_TOKEN`：Streams 管理与 `pipeline` 调用。
- `STREAM_FLUSH_INTERVAL_MS/STREAM_FLUSH_SIZE`：`token` 合并窗口与阈值。
- `STREAM_TTL_SECONDS/STREAM_TRIM_MAXLEN`：终止事件后的 `EXPIRE` 与 `XTRIM`。
- `LLM_DEBUG/DEEPSEEK_API_KEY/ZHIPUAI_API_KEY`：调试与模型接入。

## **协同方式总结**

- Producer 把好入口关（限流/幂等/审计），Worker 把好执行关（验签/配额路由/守卫/事件），SSE 把好展示与关闭关（自适应轮询与终止清理）。
- `publishEvent` 作为中心胶水：统一事件语义、批量策略与终止清理，与 SSE 的“终止即关闭”形成闭环，确保在成功或失败后 Redis I/O 能快速收敛。
- 并发锁与计数器作为系统稳态保障，配合 QStash 的“返回 200 不重试”策略，避免风暴式重试和重复计费。
