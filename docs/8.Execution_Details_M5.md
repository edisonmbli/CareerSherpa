### \#1 深入浅出地讲解 RAG

这是 M5 的“Why”和“How”。

#### A. RAG 的主要工作原理（两阶段）

RAG (Retrieval-Augmented Generation) 的核心原理很简单：**不要只依赖 LLM 记忆中的“通用知识”，而是给它“开卷考试”的机会，让它参考我们的“私有笔记”来回答问题。**

这个过程分为两个阶段：

**阶段 1：数据入库 (Ingestion) - （离线阶段，M5 的核心任务）**

这是把你的“求职宝典”（大部头书）变成 `pgvector`（数据库）中可被搜索的“知识卡片”的过程。

1.  **加载 (Load)**：我们读取你提供的 `求职干货宝典_rag_zh.md` 和 `_en.md` 文件。
2.  **分块 (Chunking)**：我们将长文档（如 Part 3）切分成更小的、**语义完整**的“块”（Chunks）。
3.  **嵌入 (Embedding)**：我们使用 M4 的 LLM 抽象层（`GLM embedding-3`）将**每一个** Chunk 转换成一个“向量”（例如 `[0.02, 0.91, -0.45, ...]`）。这个向量代表了该 Chunk 在多维空间中的“语义坐标”。
4.  **存储 (Store)**：我们将这些 Chunk 文本、它们的向量（坐标）以及元数据（`lang: 'zh'`, `category: 'customize'`） 存入 `KnowledgeEntry` 表中。`pgvector` 会为这些向量建立索引。

**阶段 2：检索与生成 (Retrieval & Generation) - （在线阶段，M8/M9 的核心任务）**

这是用户触发 `match` 或 `customize` 或 `interview` 时发生的事情。

1.  **接收查询 (Query)**：用户触发 `interview` 任务。我们的 `interview-worker` (M9) 将用户的“定制化简历” 和 “JD” 摘要成一个“查询 Q”。（例如：“如何回答‘你为什么离职？’”）
2.  **嵌入查询 (Embed Query)**：我们用**相同的** Embedding 模型将“查询 Q”也转换为一个向量。
3.  **检索 (Retrieve)**：我们拿着这个“查询向量”去 `pgvector` 数据库，执行“相似度搜索”（例如 `cosine` 距离）。
    - **指令**：“请在 `KnowledgeEntry` 表中，`WHERE lang = 'zh' AND category = 'interview'` 的条目里，找到与我这个查询向量**最接近的 3 个**（Top-K）Chunk。”
4.  **增强 (Augment)**：我们拿到这 3 个最相关的“知识片段”（例如 Part 7.4 的内容）。
5.  **生成 (Generate)**：我们调用 M4 的 `lib/llm/service.ts`，传入我们重构后的 Prompt：

    ```
    System: 你是面试教练...

    【RAG 知识库 - 面试技巧】
    """
    (检索到的 Part 7.4 的内容...)
    """

    User: 基于以下简历和 JD，请帮我准备面试...
    ```

#### B. 本项目的“分块” (Chunking) 策略

你的顾虑是对的，“分块”是 RAG 效果的生命线。

- **问题**：如果 Chunk 太大（例如 2000 token），包含了 `match` 和 `interview` 的内容，当用户只搜 `match` 时，`interview` 的内容就是“噪声”，会污染 LLM 的上下文，导致 LLM 输出跑偏。
- **策略**：我们将采用**语义分块 (Semantic Chunking)** + **元数据过滤 (Metadata Filtering)**。

<!-- end list -->

1.  **分块器 (Splitter)**：

    - 我们将使用 `LlamaIndex` (或 `LangChain`) 提供的 `MarkdownHeaderTextSplitter` 或 `SemanticSplitter`。
    - **为什么**：我们精心准备的 RAG 文档（`求职干货宝典_rag_zh.md`）是 Markdown 格式的。我们**必须**利用 `##` (H2) 和 `###` (H3) 标题作为**语义边界**来分块。例如，`Part 3.2` (XYZ 法则) 应该是一个独立的 Chunk，`Part 3.3` (动作动词) 是另一个。
    - 这比“固定 512 字符”的策略要智能得多，它保证了每个 Chunk 的“原子性”。

2.  **分块大小 (Chunk Size)**：

    - 我们将设置一个**大致的**目标尺寸，例如 `512 tokens`，但**优先**遵循 Markdown 的 H3 标题边界。

3.  **元数据 (Metadata)**：

    - 这是我们的**秘密武器**。`ingest-rag-docs.ts` 脚本在分块时，**必须**为每个 Chunk 附上元数据：
      ```json
      {
        "lang": "zh",
        "part": 3,
        "category": "customize",
        "title": "3.2 实践：如何量化成就 (X-Y-Z 法则)"
      }
      ```
    - **为什么**：因为在 M8 的 `customize-worker` 中，我们**不需要**搜索整个知识库。我们可以**精确**检索：`queryRag(..., category: 'customize')`。这能 100% 保证 LLM 拿到的都是“简历撰写”的知识，杜绝“面试技巧”的噪声。

#### C. 如何利用 LlamaIndex

`LlamaIndex` 是 M5 的核心工具库。我们将“物尽其用”，但不“过度依赖”。

**M5 (RAG 入库脚本) 将这样使用 `LlamaIndex`**：

1.  `new SimpleDirectoryReader().loadData(...)`：加载 `求职干货宝典_rag_zh.md` 文件。
2.  `new MarkdownNodeParser(...)`：这就是我们 B 节中提到的“语义分块”工具。
3.  `new PGVectorStore(...)`：连接到我们的 Neon `pgvector` 数据库。
4.  `new StorageContext.fromDefaults({ vectorStore })`：构建存储上下文。
5.  `VectorStoreIndex.fromDocuments(documents, { storageContext })`：`LlamaIndex` 会自动处理“分块”、“调用 M4 Embedding”、“存入 `KnowledgeEntry` 表”这三个步骤。

**M8/M9 (RAG 检索器) 将这样使用 `LlamaIndex`**：

1.  `new PGVectorStore()`：连接到已有的数据库。
2.  `VectorStoreIndex.fromVectorStore(vectorStore)`：加载索引。
3.  `index.asRetriever({ topK: 3, filters: ... })`：**(关键)** 创建一个检索器，并使用 `filters` 参数实现 `WHERE lang = 'zh' AND category = 'customize'` 的精确过滤。
4.  `retriever.retrieve(queryText)`：执行检索，拿到 `NodeWithScore[]` 数组，这就是我们需要的“RAG 上下文”。

---

**总结**：
我们 M5 的 RAG 策略是：

- **原理**：索引（离线）+ 检索（在线）
- **分块**：使用 LlamaIndex 的 Markdown 解析器，按“H3 标题” 进行**语义分块**。
- **检索**：通过 `lang` + `category` 进行**元数据过滤**，实现“精确制导”。

---

### \#2 RAG 输入文档如何高效读取？

#### 疑惑 1：“`category` 元数据从哪里来？”

- **答案**：你对 embedding 模型的理解完全正确。**Embedding 模型（如 `GLM embedding-3`）*只*负责数学（文本 -\> 向量），它*不*负责“主题提炼”**。
- **`category` 的来源**：\*\*是我们（开发者），在 M5 的“数据入库 (Ingestion)”阶段，\*\*手动（或通过脚本逻辑）\*\*为每个 Chunk（知识块）**强行打上的标签**。
- **解释**：

  - 这就是为什么我在「步骤 1：分析与重构策略」中要**重新组织**你的 `raw.md` 变为 8 个 Part。每 part 开头包含了 metadata 信息，例如 `category` 和 `lang`。
  - **这个新结构就是元数据的来源！** \* M5 的 `ingest-rag-docs.ts` 脚本在解析 `求职干货宝典_rag_zh.md` 时，会这样做：
    1. 按 `---` (分割线) 切分文档。
    2. 从每个 Part 的开头（H2 标题下方）用正则表达式提取出被注释的 JSON 字符串。
    3. 解析这个 JSON 得到 `category` 和 `lang`。
    4. 再对该 Part 的内容按 `###` (H3) 进行语义分块。
    5. 将 `category` 和 `lang` 元数据附加到**所有**子 Chunks 上，然后存入 `pgvector`。
    6. ……依此类推。

- 这个优化**彻底解决了“`category` 从哪里来”的疑惑，使我们的 RAG 管道变得健壮、清晰且易于维护**。
- **结论**：`category` 是我们**在入库时强加给数据的“索引标签”**，它让 M8/M9 的 Worker 在检索时可以实现\*\*“精确制导”\*\*，而不是在整个知识库中“大海捞针”。

#### 疑惑 2：“`customize` 任务也需要 RAG”

- **答案**：你 100% 是对的。**`customize` 任务*必须*使用 RAG**。
- **解释**：
  - `match` 任务，需要 RAG（`Part 4: 匹配度分析`）来\*\*“分析”\*\*。
  - `customize` 任务，需要 RAG（`Part 3: 简历定制化撰写`）来\*\*“创作”\*\*。
  - 如果 `customize` 任务没有 RAG，LLM 只能“猜”什么是好的简历。但有了 `Part 3` 的 "XYZ 法则" 和 "动作动词" 作为上下文，LLM 就能\*\*“有章法地”**、**“专业地”\*\*去重写简历，这正是我们的核心价值。
  - 因此，`match`, `customize`, 和 `interview` **三者都会调用 RAG**，只是它们会查询**不同 `category`** 的知识。

### \#3 存量代码可用性分析

**目标**：评估 Prototype 中留下的 RAG 相关代码，并决定我们的“重构策略”（保留、废弃、重构）。

**分析的存量文件**：

1.  `prisma/schema.prisma` (RAG 部分)
2.  `lib/dal/vector-store.ts`
3.  `lib/llm/service.ts`, `lib/llm/task-router.ts` (Embedding 调用)
4.  `lib/rag/document-vectorizer.ts`
5.  `lib/rag/index.ts`
6.  `lib/rag/query.ts`

**评估结果与重构策略**：

**1. `prisma/schema.prisma` (RAG 表)**

- **存量**：`KnowledgeEntry`, `Document`, `DocumentSourceType`。
- **分析**：这些表的设计**过于复杂**，且与我们的新 `schema`（文档 2.4） **完全冲突**。
  - 它们包含了 `userId`，试图构建“用户私有知识库”，这**不是**我们 MVP（全局“求职宝典”）的目标。
  - 我们的新 schema（M1 已重置）中，已经定义了一个更简单、更干净的 `KnowledgeEntry` 表（已在 M4 补丁中被 `LlmUsageLog` 替换，但我们在 M1 重置时已经使用了文档 2.4 的 `KnowledgeEntry`）。
- **决策**：**废弃**。M1 的 `prisma migrate reset` 已经清理了这些表。M5 将使用 M1 迁移时创建的、文档 2.4 定义的 `KnowledgeEntry` 表。

**2. `lib/llm/service.ts` & `task-router.ts` (Embedding 调用)**

- **存量**：M4 已构建。
- **分析**：M4 的 `lib/llm/providers.ts` 里**缺少**一个专门的 `Embedding` 模型实例（例如 `GLM embedding-3` 或 `text-embedding-3-small`）。
- **决策**：**重构** `lib/llm/providers.ts`，添加一个 `getEmbeddingModel()` 方法。

**3. `lib/rag/*.ts` (Prototype 的 RAG 逻辑)**

- **存量**：
  - `document-vectorizer.ts`
  - `index.ts`
  - `query.ts`
  - `lib/dal/vector-store.ts`
- **分析**：
  - `document-vectorizer.ts` 和 `vector-store.ts` 包含了很多**原始的 Prisma + pgvector SQL 操作**。
  - 它们**没有**使用 `LlamaIndex`，而是“手搓”了向量化和入库逻辑。
  - 这与我们的 M5 规划（使用 `LlamaIndex` 来处理分块和入库） **相冲突**。
- **决策**：**全部废弃**。
  - **AI IDE 指令**：**删除** `lib/rag/` 目录和 `lib/dal/vector-store.ts`。
  - M5 将按照 \#1 中定义的 `LlamaIndex` 策略**重新创建**这些文件。

**存量分析总结：**

- **数据库**：M1 已重置为新 Schema。
- **RAG 逻辑 (Prototype)**：**全部废弃**。
- **LLM 抽象 (M4)**：**保留**，但需**扩展**以支持 Embedding。

### \#4 具体重构步骤指南

M4（LLM 抽象层） 已完成，非常好。

我们正式启动 **M5：AI 基建 - RAG 系统**。

你对 M5 的精细化推进方案（\#1 到 \#4）非常专业。我们已经完成了 \#1 (RAG 原理) 和 \#2 (文档调优)，并澄清了 `customize` 和 `category` 的逻辑。

现在，我们继续推进。

---

### \#3 存量相关代码分析

**目标**：评估 Prototype 遗留的 RAG 代码，并制定“废弃”与“重构”策略。

**核心架构冲突分析：**
你提供的最新 `schema.prisma` 状态暴露了一个关键的架构冲突：

1.  **`KnowledgeEntry` 表**：这是我们 M1 + M4 规划的\*\*“全局 RAG”\*\*（求职宝典）。
2.  **`Document` 表**：这是 Prototype 遗留的\*\*“用户私有 RAG”\*\*（用于 embedding 用户自己的 `resume` 和 `job_description`）。

**决策：**

- **MVP 范围**：M5（本里程碑）**只关注“全局 RAG”**（`KnowledgeEntry`）。
- “用户私有 RAG”（`Document`）是一个**未来**的高级功能（例如“根据我的简历自动回答面试问题”），**不在本次重构范围内**。
- 因此，所有与 `Document` 表相关的 Prototype 代码**必须被废弃**。

---

### \#4 具体重构步骤指南

**目标**：构建、入库并测试我们的“全局求职宝典” RAG 系统。
**规范**：严格遵循 `project rules`（Rule 6 `LlamaIndex`, Rule 8 `DAL`）和 **模型决策**（`glm embedding-3`, 2048 维）。

---

#### 任务 0：(项目 Owner) Schema 修正与清理 - 当前已完成！

**目标**：将 `schema.prisma` 清理到“M5 就绪”状态，并**正确设置向量维度**。
**执行者**：**项目 Owner**（你）。

**1. 手动修改 `prisma/schema.prisma`**

- **删除**：**删除** `Document` 模型和 `DocumentSourceType` 枚举（它们是 Prototype 遗留的）。
- **修正 `KnowledgeEntry`**：这是 M5 唯一需要的 RAG 表。
- **`KnowledgeEntry` 最终版 Schema (以此为准)**：

  ```prisma
  // AI IDE: This is the ground truth for M5
  model KnowledgeEntry {
    id        String                 @id @default(cuid())
    title     String?                // H3 标题
    content   String                 // 分块后的文本

    // 修正：根据 Zhipu 官方文档，维度为 2048
    embedding Unsupported("vector(2048)")?

    lang      String                 // "zh" or "en"
    source    String?                // e.g., "求职干货宝典_rag_zh.md"
    category  String?                // "customize", "match", "interview_qa" etc.
    isPublic  Boolean                @default(true) @map("is_public")
    createdAt DateTime               @default(now()) @map("created_at")
    updatedAt DateTime               @updatedAt @map("updated_at")

    // DROPPED: userId and user relation (M5 is global RAG)

    @@index([lang, category, isPublic])
    @@map("knowledge_entries")
    @@schema("public")
  }
  ```

**2. (你来执行) 迁移与清理**

1.  **清理 Prototype 代码**：**手动删除**以下所有与“用户私有 RAG”相关的文件：
    - `rm lib/dal/vector-store.ts`
    - `rm lib/rag/document-vectorizer.ts`
    - `rm lib/rag/index.ts`
    - `rm lib/rag/query.ts`
2.  **应用 Schema 变更**：
    ```bash
    pnpm dlx prisma migrate dev --name refactor-rag-schema
    ```
3.  **重新生成 Client**：
    ```bash
    pnpm dlx prisma generate
    ```

---

#### 任务 1：(AI IDE) 安装依赖 & 扩展 M4

- **目标**：安装 `LlamaIndex` 和 Zhipu 相关依赖，并扩展 M4 以**正确**支持 `glm embedding-3`。

- **1.1: (AI IDE) 安装 LlamaIndex 与 Zhipu 依赖**

  ```bash
  pnpm add llamaindex @langchain/community
  # LlamaIndex 的 Zhipu embedding 可能需要 @zhipuai/zhipuai
  pnpm add @zhipuai/zhipuai
  ```

- **1.2: (AI IDE) 扩展 M4 的 Provider**

  - **文件**：`lib/llm/providers.ts`
  - **动作**：**替换** OpenAI embedding 为 `GLM embedding-3`。

  <!-- end list -->

  ```typescript
  // AI IDE: Use this *updated* content for lib/llm/providers.ts

  import { ChatOpenAI } from '@langchain/openai'
  import {
    ChatZhipuAI,
    ZhipuAIEmbeddings,
  } from '@langchain/community/chat_models/zhipuai' // LangChain
  import { BaseChatModel } from '@langchain/core/language_models/chat_models'
  import { BaseEmbedding } from '@langchain/core/embeddings'
  import { ZhipuAIEmbedding } from 'llamaindex' // LlamaIndex

  // 定义我们的 App 支持的模型 ID
  export enum ModelId {
    DEEPSEEK_CHAT = 'deepseek-chat',
    DEEPSEEK_REASONER = 'deepseek-reasoner',
    GLM_4_5_FLASH = 'glm-4.5-flash',
    GLM_4_VISION = 'glm-4v',

    // 修正：使用 Zhipu Embedding
    GLM_EMBEDDING_3 = 'embedding-3',
  }

  // ... (PROVIDER_MAP for Chat Models remains the same as M4) ...
  const PROVIDER_MAP: Record<string, () => BaseChatModel> = {
    [ModelId.DEEPSEEK_CHAT]: () =>
      new ChatOpenAI({
        modelName: 'deepseek-chat',
        apiKey: process.env.DEEPSEEK_API_KEY,
        baseURL: 'https://api.deepseek.com/v1',
        temperature: 0.1,
      }),
    [ModelId.DEEPSEEK_REASONER]: () =>
      new ChatOpenAI({
        modelName: 'deepseek-reasoner',
        apiKey: process.env.DEEPSEEK_API_KEY,
        baseURL: 'https://api.deepseek.com/v1',
        temperature: 0.1,
      }),
    [ModelId.GLM_4_5_FLASH]: () =>
      new ChatZhipuAI({
        modelName: 'glm-4.5-flash',
        apiKey: process.env.ZHIPU_API_KEY,
        temperature: 0.1,
      }),
    [ModelId.GLM_4_VISION]: () =>
      new ChatZhipuAI({
        modelName: 'glm-4v',
        apiKey: process.env.ZHIPU_API_KEY,
        temperature: 0.1,
      }),
  }

  export const getModel = (modelId: ModelId): BaseChatModel => {
    // ... (getModel function remains the same)
    const modelFactory = PROVIDER_MAP[modelId]
    if (!modelFactory) {
      throw new Error(`Invalid Chat ModelId: ${modelId}`)
    }
    return modelFactory()
  }

  /**
   * 【修正】M5 任务：获取 Embedding 模型 (LlamaIndex 兼容版)
   */
  export const getLlamaIndexEmbeddingModel = () => {
    // 规范：使用 glm embedding-3
    // LlamaIndex 的 ZhipuAIEmbedding 默认使用 embedding-3
    return new ZhipuAIEmbedding({
      apiKey: process.env.ZHIPU_API_KEY,
      // 维度 2048 是此模型的默认值，LlamaIndex 会自动处理
    })
  }

  /**
   * 【修正】M5 任务：获取 Embedding 模型 (LangChain 兼容版)
   */
  export const getLangChainEmbeddingModel = (): BaseEmbedding => {
    // 规范：使用 glm embedding-3
    return new ZhipuAIEmbeddings({
      apiKey: process.env.ZHIPU_API_KEY,
      modelName: ModelId.GLM_EMBEDDING_3, // 'embedding-3'
      // 维度 2048 是此模型的默认值
    })
  }
  ```

---

#### 任务 2：(AI IDE) RAG 入库脚本 (M5.1)

- **目标**：创建 `scripts/ingest-rag-docs.ts`，读取调优后 的 Markdown，按 H3 分块，解析元数据，并存入 `pgvector`。
- **文件**：`scripts/ingest-rag-docs.ts`

<!-- end list -->

```typescript
// AI IDE: Create this file at scripts/ingest-rag-docs.ts
import {
  SimpleDirectoryReader,
  MarkdownNodeParser,
  serviceContextFromDefaults,
  VectorStoreIndex,
  PGVectorStore,
  storageContextFromDefaults,
} from 'llamaindex'
import { db } from '@/lib/prisma'
import { getLlamaIndexEmbeddingModel } from '@/lib/llm/providers'

// 元数据正则表达式 (用于 #2 调优)
const METADATA_REGEX = //;
  async function main() {
    console.log('Starting RAG ingestion process...')

    // --- 1. 定义 RAG 文档源 ---
    // (Owner 需创建此目录并放入 md 文件)
    const RAG_DOCS_PATH = './rag_documents'
    const vectorStore = new PGVectorStore({
      // LlamaIndex v0.4+ (Project Rule 6.2)
      db: db,
      tableName: 'KnowledgeEntry', // 对应 M1 修正后的 Schema
      vectorColumnName: 'embedding', // 对应 Schema
    })

    // --- 2. 清理旧数据 (确保幂等性) ---
    console.log('Cleaning old knowledge base entries...')
    // 必须使用 Prisma raw query 来 TRUNCATE，因为 LlamaIndex 不会清空
    await db.$executeRawUnsafe(
      `TRUNCATE TABLE "public"."knowledge_entries" RESTART IDENTITY CASCADE;`
    )
    console.log('Table "knowledge_entries" cleared.')

    // --- 3. 加载文档 ---
    const reader = new SimpleDirectoryReader()
    const documents = await reader.loadData(RAG_DOCS_PATH)
    console.log(`Loaded ${documents.length} document(s) from ${RAG_DOCS_PATH}`)

    // --- 4. 定义分块与服务上下文 ---
    const nodeParser = new MarkdownNodeParser({
      // 规范 #1.B: 按 H3 标题语义分块
      splitOn: ['---', '##', '###'],
      includeMetadata: true,
    })

    // 规范 #1.C: 使用 M4 修正后的 glm embedding-3
    const embedModel = getLlamaIndexEmbeddingModel()

    const serviceContext = serviceContextFromDefaults({
      nodeParser,
      embedModel,
      // 默认 chunk size
      chunkSize: 512,
      chunkOverlap: 50,
    })

    const storageContext = await storageContextFromDefaults({ vectorStore })

    // --- 5. 处理并注入元数据 (关键!) ---
    console.log('Parsing nodes and injecting metadata...')
    let currentCategory = 'unknown'
    let currentLang = 'en'

    for (const doc of documents) {
      const nodes = nodeParser.getNodesFromDocuments([doc])

      for (const node of nodes) {
        const content = node.getContent()

        // 检查元数据标签 (规范 #2)
        const metadataMatch = content.match(METADATA_REGEX)
        let metadata: any = {}
        if (metadataMatch) {
          try {
            metadata = JSON.parse(metadataMatch[1])
            currentCategory = metadata.category || currentCategory
            currentLang = metadata.lang || currentLang
          } catch (e) {
            console.warn(`Failed to parse metadata: ${metadataMatch[1]}`)
          }
        }

        // 提取 H3 标题作为 node title
        const titleMatch = content.match(/^###\s+(.*)/m)

        // 附加元数据 (规范 #1.B)
        node.metadata = {
          ...node.metadata,
          category: metadata.category || currentCategory,
          lang: metadata.lang || currentLang,
          title: titleMatch ? titleMatch[1] : 'Untitled',
          source: doc.id_ || 'unknown',
        }

        // 清理原始文本中的元数据标签，防止污染 embedding
        node.setContent(content.replace(METADATA_REGEX, '').trim())
      }

      // --- 6. 执行索引 (Embedding + 存储) ---
      console.log(
        `Ingesting ${nodes.length} nodes from ${doc.id_} (lang: ${currentLang}, category: ${currentCategory})...`
      )
      await VectorStoreIndex.fromNodes(nodes, {
        storageContext,
        serviceContext,
      })
    }

    console.log('RAG ingestion complete!')
  }

main()
  .catch((e) => {
    console.error(e)
    process.exit(1)
  })
  .finally(async () => {
    await db.$disconnect()
  })
```

---

#### 任务 3：(AI IDE) RAG 检索器 (M5.2)

- **目标**：创建 `lib/rag/retriever.ts`，为 M8/M9 的 Worker 提供“精确制导”的 RAG 上下文。
- **文件**：`lib/rag/retriever.ts` (新建，替换已删除的原型 `lib/rag/query.ts`)

<!-- end list -->

```typescript
// AI IDE: Create this file at lib/rag/retriever.ts
import {
  PGVectorStore,
  VectorStoreIndex,
  serviceContextFromDefaults,
  MetadataFilters,
} from 'llamaindex';
import { db } from '@/lib/prisma';
import { getLlamaIndexEmbeddingModel } from '@/lib/llm/providers';
import { Locale } from '@/i18n-config';

// 缓存索引，避免 Vercel Serverless 冷启动时重复创建
let_index: VectorStoreIndex | null = null;

async function getIndex() {
  if (_index) return _index;

  const vectorStore = new PGVectorStore({
    db: db,
    tableName: 'KnowledgeEntry',
    vectorColumnName: 'embedding',
  });

  const serviceContext = serviceContextFromDefaults({
    // 规范 #1.C: 检索器必须使用与入库时相同的 embedding 模型
    embedModel: getLlamaIndexEmbeddingModel(),
  });

  console.log('Initializing RAG index from existing vector store...');
  _index = await VectorStoreIndex.fromVectorStore(vectorStore, serviceContext);
  return _index;
}

/**
 * RAG 检索器 (M5.2 核心)
 * @param queryText 用户的查询（例如 JD 摘要）
 * @param lang 'zh' | 'en'
 * @param categories 允许的一个或多个 RAG 类别
 * @param topK 返回多少个 Chunks
 */
export async function queryRag(
  queryText: string,
  lang: Locale,
  categories: string[], // 允许多个 category
  topK: number = 3
): Promise<string> {
  if (categories.length === 0) return ''; // 如果没有指定类别，则不检索

  try {
    const index = await getIndex();

    // 规范 #1.B: 使用元数据进行“精确过滤”
    const filters: MetadataFilters = {
      filters: [
        { key: 'lang', value: lang, operator: '==' },
        {
          key: 'category',
          value: categories,
          // 允许多选
          operator: 'in'
        },
      ],
    };

    const retriever = index.asRetriever({
      topK: topK,
      filters: filters,
    });

    const nodes = await retriever.retrieve(queryText);

    if (nodes.length === 0) {
      console.warn(`RAG: No context found for query "${queryText}" in categories [${categories.join(',')}]`);
      return ''; // 返回空上下文
    }

    // 格式化 RAG 上下文，注入到 M4 Prompt
    return nodes
      .map((node) => `[相关知识: ${node.metadata.title || ''}]\n${node.getContent()}`)
      .join('\n\n---\n\n');

  } catch (error) {
    console.error(`[RAG Retriever Error]: ${error.message}`);
    // 失败时安全降级，返回空上下文
    return '';
  }
}
```

---

#### 任务 4：(项目 Owner) 执行 RAG 入库 (M5.3)

- **目标**：填充 `pgvector` 数据库。
- **动作 (Owner)**：
  1.  （如 M1 所述）确保 `pgvector` 已启用：`CREATE EXTENSION IF NOT EXISTS vector;`。
  2.  创建 `rag_documents/` 目录。
  3.  将你已添加元数据 的 `求职干货宝典_rag_zh.md` 和 `求职干货宝典_rag_en.md` 放入该目录。
  4.  运行 AI IDE 创建的脚本：
      ```bash
      pnpm dlx tsx scripts/ingest-rag-docs.ts
      ```
  5.  （验证）使用 `prisma studio` 检查 `KnowledgeEntry` 表，确保数据、`embedding`（应为 2048 维）、`category`、`lang` 均已填入。

---

#### 任务 5：(AI IDE) RAG 检索单元测试 (M5 DoD)

- **目标**：**(解决 你的测试需求)** 验证 `queryRag` 检索器能否**精确**命中我们想要的知识。
- **文件**：`tests/rag/retriever.test.ts` (AI IDE 需创建此文件)

<!-- end list -->

```typescript
// AI IDE: Create this file at tests/rag/retriever.test.ts
import { describe, it, expect, beforeAll } from 'vitest'
import { queryRag } from '@/lib/rag/retriever'
import { db } from '@/lib/prisma'

// M5 DoD: 确保 RAG 已入库
beforeAll(async () => {
  const count = await db.knowledgeEntry.count()
  if (count === 0) {
    throw new Error(
      'Database not ingested! Run `pnpm tsx scripts/ingest-rag-docs.ts` before testing.'
    )
  }
})

describe('M5 RAG Retriever Service', () => {
  // 测试 1: `match` 任务 (多 category 查询)
  it('should retrieve "match" and "match_intent" context for a JD query', async () => {
    const query = '分析 JD 和简历的匹配度'
    // 规范: match 任务同时需要 "匹配度分析" 和 "求职意向表达" 知识
    const context = await queryRag(query, 'zh', ['match', 'match_intent'])

    console.log('[Match Test Context]:\n', context)
    expect(context).toBeDefined()
    expect(context).not.toBe('')
    expect(context).toContain('JD') // 应该包含 Part 4 的内容
    expect(context).toContain('H-V-C') // 应该包含 Part 5 的内容
    expect(context).not.toContain('STAR 法则') // 不应包含 Part 7 的内容
  })

  // 测试 2: `customize` 任务 (单一 category 查询)
  it('should retrieve "customize" context for a resume writing query', async () => {
    const query = '如何量化我的工作成就'
    const context = await queryRag(query, 'zh', ['customize'])

    console.log('[Customize Test Context]:\n', context)
    expect(context).toBeDefined()
    expect(context).not.toBe('')
    expect(context).toContain('X-Y-Z') // 应该包含 Part 3 的内容
    expect(context).not.toContain('职业锚') // 不应包含 Part 1 的内容
  })

  // 测试 3: `interview` 任务 (多 category 查询, 英文)
  it('should retrieve "interview_*" context for an interview query', async () => {
    const query = 'how to answer about my weakness'
    // 规范: interview 任务同时需要 "自我介绍" 和 "QA技巧" 知识
    const context = await queryRag(query, 'en', [
      'interview_intro',
      'interview_qa',
    ])

    console.log('[Interview Test Context]:\n', context)
    expect(context).toBeDefined()
    expect(context).not.toBe('')
    expect(context).toContain('weakness') // 应该包含 Part 7 的内容
    expect(context).toContain('P-P-F') // 应该包含 Part 6 的内容
    expect(context).not.toContain('Deconstructing the JD') // 不应包含 Part 4 的内容
  })
})
```

---

### 交付记录

已完成 M5 的 1/2/3 三个实现项：新增检索模块、导入脚本、与单元测试，并通过全量测试验证。

新增内容

- lib/rag/retriever.ts

  - 提供 queryRag(queryText, { lang, category, topK, minScore }) 与 queryRagByCategory(...) 。
  - 使用 GLM Embedding-3 生成查询向量（2048 维），在 knowledge_entries 上用 pgvector 的余弦距离 <=> 检索。
  - 统一将距离归一化为得分 score ∈ [0,1] （ score = 1 - distance/2 ），支持 lang 与 category 元数据过滤，并默认仅读取 is_public = true 。
  - 结果按得分降序返回，包含 id/title/content/category/lang/score 。

- scripts/ingest-rag-docs.ts

  - 从 rag_documents 目录读取 .md/.pdf ，按标题 + 段落进行 Chunking（默认 maxChars=1200 ），调用批量嵌入并逐条写入 knowledge_entries 。
  - CLI 参数： --lang en|zh 、 --category <string> 、 --dry （预览不入库）、 --max <chars> 。
  - 写库时使用 pgvector 字面量插入（ [e1,...,e2048]::vector ），并设置 is_public=true 。

- tests/rag/retriever.test.ts

  - 通过 vi.mock 伪造嵌入与查询结果，验证： - score 归一化在 0..1 ； - 结果按相似度降序； - 可选字段 title/category 的准确映射； - minScore 过滤生效。
    如何使用（任务 4：你执行导入）

- 准备“需求原文”双语文档，放到项目根目录的 rag_documents/ （已在 .gitignore 排除）。
- 英文导入：
  - tsx scripts/ingest-rag-docs.ts --lang en --category job_requirements
- 中文导入：
  - tsx scripts/ingest-rag-docs.ts --lang zh --category job_requirements
- 预览不写库：
  - tsx scripts/ingest-rag-docs.ts --lang en --category job_requirements --dry
- 可调整块大小：

  - tsx scripts/ingest-rag-docs.ts --lang en --category job_requirements --max 1500
    检索用法示例

- 在服务侧调用（Server Components/Server Actions 中）：

  - import { queryRag } from '@/lib/rag/retriever'
  - const items = await queryRag('skills: react typescript', { lang: 'en', category: 'job_requirements', topK: 8, minScore: 0.35 })
  - 结果为降序 Top‑K， score 可直接用于排序与阈值过滤。
    设计与实现要点

- 向量检索
  - 使用 <=> 余弦距离（越小越相似），并标准化得分 score = 1 - distance/2 ，统一到 0..1 便于上层逻辑。
  - 查询 SQL 通过 Prisma.sql 组合，向量以字面量传入。 WHERE 支持 lang 、可选 category 、并固定 is_public=true 。
- Chunking 策略
  - Markdown 以标题作为语义锚点，结合段落与最大长度限制；PDF 走 pdf-parse 抽取纯文本。
  - Chunk title 优先最近标题，不存在则用文件名兜底。
- 嵌入
  - 复用既有 runEmbedding / runEmbeddingBatch （GLM Embedding-3，2048 维）。不额外引入 Zhipu SDK，沿用已实现的 REST 调用方案。
- DAL 约束

  - 脚本为离线导入工具，直接使用 prisma 写库（与现有审计/迁移模块一致）。线上检索走 lib/rag/retriever.ts ，仅执行只读查询。
    测试与验证

- 已运行 npm test -s ：共 46 个测试全部通过，无类型错误。
- 新增两项 RAG 用例覆盖归一化与排序、可选字段映射、阈值过滤。
  你下一步

- 将“需求原文”双语文档放入 rag_documents/ 并执行上述两次脚本（ en 与 zh ）。
- 执行后，我可以用 queryRag 做一次读取验证，确保 DoD 达成。
  DoD（验收）

  - KnowledgeEntry 表已有导入的语义块数据（含 embedding/lang/category/is_public ）。
  - queryRag 对“需求原文”语义查询能返回相关中文/英文文本块，得分与排序合理。
    补充说明

- 依赖核验
  - llamaindex 与 @langchain/community 已存在。嵌入使用既有 REST Provider，无需 zhipu SDK。
  - 安全提醒
  - 本次未执行任何危险操作（未重置数据库、未变更 schema）。如需重置，请先确认再执行 npx prisma migrate reset --force 。
    如需将检索封装为 Server Action 或把导入脚本扩展为分类批量导入（按子目录自动识别 en/zh 、多分类），我可以继续完善。

---

### 附录：DAL 封装与验证（M5.0）

为确保所有数据库读写统一走 DAL（遵循项目 Rule 4），并验证 Prisma + pgvector 的稳定性，新增如下封装与本地验证脚本。

【DAL 封装】`lib/dal/knowledgeEntries.ts`

- 写入：`createKnowledgeEntry(params)` 使用 Prisma 的 `$queryRaw` 进行插入，向量以文本字面量 `"[e1,...,e2048]"::vector` 传入，并 `RETURNING` 关键字段。
- 检索：`findSimilarKnowledgeEntries(queryEmbedding, filters, topK, minScore)` 通过 `embedding <-> queryVec` 执行余弦距离排序，并统一得分 `score = 1 - distance / 2` 到 `0..1`，支持 `lang/category/is_public` 过滤（默认 `is_public = true`）。
- 维度校验：严格断言 `embedding.length === 2048`，与 `GLM embedding-3` 输出维度保持一致。

【本地验证脚本】`scripts/verify-knowledge-dal.ts`

- 作用：在本地环境插入一条测试记录（确定性向量），执行相似度检索（带元数据过滤），并在结尾清理测试数据。
- 运行：`pnpm tsx scripts/verify-knowledge-dal.ts`
- 期望输出：显示插入的 `id/title/lang/category`，以及 Top‑K 检索结果（含 `score`），`score` 应在 `0..1` 且按降序排列。

【环境与连接建议】

- `DATABASE_URL` 使用 Neon 的 **pooled endpoint**（示例：`postgresql://USER:PASS@ep-xxxx-pooler.neon.tech:5432/DB?sslmode=require`）。
- 运行时使用 **Node.js**（本地 `tsx`），避免 Edge/WebSockets 驱动与 Prisma 冲突。
- 若出现 `PrismaClientInitializationError`（无法连接）：
  - 检查网络/DNS、`sslmode=require`、端口 5432 开放；
  - 用 `psql` 或 `npx prisma db pull` 验证连通性；
  - 如仍异常，暂缓脚本，优先在应用内（Server Actions）验证读写路径。

【与 LlamaIndex 的配合】

- M5 的入库脚本（`scripts/ingest-rag-docs.ts`）走 LlamaIndex 的向量化与批量写入；线上检索（`lib/rag/retriever.ts`）走 LlamaIndex 检索器。
- DAL 作为“最低层保障”：在需要更精细控制或排障时（特别是写入与直接 SQL 检索），统一调用 `lib/dal/knowledgeEntries.ts`。

【结论】

- 结合 Google AI mode 的结论：Prisma 对 `pgvector` 的读写已可用；但**向量检索**仍需原生 SQL。当前封装与脚本即遵循此实践，保证写入可靠、检索可控，并与 LlamaIndex 工作流兼容。

### 交付后的讨论 - 理想的技术方案

（无法实现，因官方 PGVectorStore 实现缺失，module cannot found）

你说得对：我之前用“运行时动态导入”只是为了先解除 ERR_PACKAGE_PATH_NOT_EXPORTED 的阻塞，让 dry-run 能继续推进，不是一个长期的规范解法。就项目的 RAG 基建而言，我们需要把“写入向量”和“检索”都稳定在 LlamaIndex 的标准 VectorStore 上，这样后续链式编排、过滤、Retriever 等能力才能保持一致且可维护。

当前已知的困难

- 包结构不一致：你项目锁定的 llamaindex@^0.12.0 并不导出文档里示例的 llamaindex/storage/vectorStore/PrismaVectorStore 子路径。我们在 node_modules 里确认不到该导出（有变更历史，但顶层并未暴露出 PrismaVectorStore ）。
- Prisma + vector 类型：Prisma 本身不支持 vector(2048) 的内置类型，通常需要 raw SQL 或外部适配器。LlamaIndex 曾经的 Prisma 适配器现在未必可用，容易和当前版本不匹配。
- LlamaIndex 的 PGVectorStore：CHANGELOG 里多次提到 PGVectorStore ，但目前包的类型导出在你的安装版本上不直接可见。通常它要求 pg / pg.Pool 连接（或等价的 Postgres 客户端）；你的项目没有安装 pg ，因此即使存在，也无法初始化。
- LangChain 的 Prisma/PGVector 实现：我们在 @langchain/community 找到了 PrismaVectorStore 和 PGVectorStore 的实现，但这不等价于 LlamaIndex 的 VectorStore。强行混用会造成“写入在 LangChain，检索在 LlamaIndex”的割裂，违反你希望统一在 LlamaIndex 的初衷。
  为什么我先用动态导入

- 这是一个“先把编译期错误绕开，让脚本能跑”的止痛贴，确保 binder 整合、chunk 一致性优先落地，团队能继续完成任务 4 的其它部分。
- 它没有改变长期方向：真实写库仍然优先走 LlamaIndex 分支，失败才回退 DAL。只是避免 dry-run 时硬解析到不存在的子路径，导致脚本直接崩溃。
  更稳妥、符合项目规范的方案（专家级长期方案）
  目标：以 LlamaIndex 为统一的向量存储入口，Neon + pgvector 为落地，Writer 与 Retriever 共用一个 Store，保留 DAL 作为故障兜底。

建议的技术路径

- 采用 LlamaIndex 的 PGVectorStore 作为唯一的 VectorStore 实现。
  - 这与“Neon + pgvector”的架构天然匹配，避免 Prisma 的类型不兼容。
  - 连接建议使用标准 pg 客户端（ pg.Client 或 pg.Pool ）并传入 DATABASE_URL ；Neon 连接字符串与 pgvector 兼容性好。
- lib/rag/vectorStore.ts 改造为 PGVectorStore：
  - 运行时初始化 PGVectorStore ，参数包含：
    - client 或 connectionString （指向 Neon）
    - tableName: 'knowledge_entries'
    - textColumn: 'content'
    - vectorColumn: 'embedding' （维度 2048）
    - 可选 metadataKeys: ['id','title','lang','category','is_public','source','created_at','updated_at']
  - 对于 LlamaIndex 的过滤接口，确保 PGVectorStore 支持 filters （比如按 lang 、 category 、 is_public ）。
- 写入脚本 scripts/ingest-rag-docs.ts ：
  - 在 writer=llama 分支，将 toInsert 统一转换为 LlamaIndex 的 Document ，将元数据绑定后的 title/lang/category/source/is_public 存入 metadata。
  - 调用 PGVectorStore.addDocuments(docs, embedModel) 完成向量写入；embed 使用 getLlamaIndexEmbeddingModel() （GLM embedding-3，维度 2048）。
  - 继续保留 DAL 兜底：如果 PGVectorStore 写入失败（连接/权限/兼容性），自动回退到 DAL 批量写入，保证流程不中断。
- 检索端 lib/rag/retriever.ts ：

  - 用 VectorStoreIndex.fromVectorStore(store) + index.asRetriever({ topK, filters }) ，filters 包含 lang 、 category 、 is_public 。
  - 将检索结果按得分归一化输出为 RagSearchResult 。
    实施步骤与风险控制

- 依赖安装：
  - 安装 pg 库（LlamaIndex 的 PGVectorStore 标准依赖）。使用 DATABASE_URL 环境变量与 Neon 建连。
- 验证流程：
  - 写入：选择 writer=llama ，验证成功写入到 knowledge_entries ，embedding 列维度正确，metadata 字段持久化。
  - 检索：调用 queryRag ，确认 lang/category/is_public 过滤生效，并返回 Top-K 的结构化结果。
  - 回退路径：手动制造连接失败场景，验证脚本降级到 DAL 写入时依然成功。
- 回滚策略：
  - 所有改动都只在 lib/rag/vectorStore.ts 和写入脚本分支，若遇到 LlamaIndex 包版本不兼容，可回滚到当前 DAL-only 的实现，业务不受影响。
- 兼容性提醒：

  - PGVectorStore 与嵌入维度要匹配（2048）；若切换模型，需同步调整表结构与维度，避免距离计算异常。
  - Neon 连接若启用 SSL，确保 pg 客户端配置正确。
    为什么这套方案更好

- 完全符合“LlamaIndex + pgvector”的架构初心；Writer/Reader 同栈，无技术债。
- 解决 Prisma 与 pgvector 类型不兼容的问题，减少隐性维护成本。
- 保留 DAL 兜底，线上稳定性更高；错误路径可控，调试更容易。
- 统一元数据过滤，避免不同库之间的 filter 语义差异。
  下一步我可以直接推进以下改造（不会只停留在绕过）：

- 在 lib/rag/vectorStore.ts 内实现 PGVectorStore 初始化（动态从 llamaindex 顶层模块获取），并配置 Neon 连接。
- 在 scripts/ingest-rag-docs.ts 的 writer=llama 分支，改为调用 PGVectorStore 的 addDocuments 。
- 在 lib/rag/retriever.ts 保持 VectorStoreIndex 检索逻辑，验证 filters 生效。
- 提供一组 Vitest 的最小验证用例（针对 retriever），并给出运行命令。
  如果你认可这套方向，我将按上述步骤直接落地实现，并把必要的依赖与配置文件调整一起完成；改完后，用你的数据跑一遍 ingest + query 检验，保证 RAG 基建完整闭环。

### 实际方案与妥协说明（M5 最终实现形态）

【总体策略】

- 写入走自研 `pgVectorAdapter` + DAL：向 `public.knowledge_entries` 用原生 SQL 插入文本与向量，手动设置 `updated_at=NOW()`，避免 Prisma `@updatedAt` 在 insert 时为空导致约束错误。
- 检索走 LlamaIndex 检索器：`lib/rag/retriever.ts` 使用 `VectorStoreIndex`/`asRetriever` 风格并结合 `filters`（`lang`、`category`、`is_public`）返回 `RagSearchResult`。
- 分块与元数据绑定采用“经典分块器”而非 `SentenceSplitter`：保留标题/段落结构，识别 `<!-- RAG_METADATA: { lang, category } -->` 并仅用于绑定，不写入正文；同时规避横线分隔符进入正文导致脏数据。
- 嵌入调用统一入口并写表日志：导入脚本改为调用 `runEmbeddingBatch`，自动写入 `llm_usage_logs`；若日志写入失败不阻塞导入（降级容错）。

【关键实现】

- `lib/rag/chunking.ts`

  - 新增 `chunkMarkdownClassic`：
    - 识别标题行并按章节分块；
    - 过滤横线分隔符（`---`、`___`、`***`、`—`、`–`），不进入正文；
    - `normalizeChunkBody` 合并英文段落中的软换行（保留列表项与段落空行），避免英文章节被拆成多行；
    - 注释元数据作为优先值，CLI 的 `--lang/--category` 仅兜底，不强制覆盖。
  - 保留 `chunkUsingLlamaIndex` 作为备选；其传参在 `exactOptionalPropertyTypes: true` 下做了布尔与字符串规范化，避免 `undefined` 赋值报错。

- `lib/rag/pgVectorAdapter.ts`

  - 插入语句显式带上 `updated_at = NOW()`，解决 not-null 约束；
  - 统一写入 `content/title/lang/category/source/is_public/embedding`，维度 2048 与 GLM-3 对齐。

- `lib/dal/knowledgeEntries.ts`

  - 严格校验嵌入维度（2048）；
  - 调用 vectorStore.add 前后统一元数据结构，保障检索过滤语义一致。

- `lib/llm/service.ts` 与 `lib/dal/llmUsageLog.ts`
  - 嵌入入口使用 `runEmbeddingBatch`，批量聚合 `usage.totalTokens` 并写入 `llm_usage_logs`（`taskTemplateId=rag_embedding`）；
  - 日志写入失败时仅警告不抛错，避免阻塞导入流程；
  - 记录 `provider/modelId/inputTokens/latencyMs/isSuccess` 等字段，便于后续成本分析与限流。

【为何妥协】

- LlamaIndex 的 Prisma/PGVectorStore 在当前版本不可用或导出路径缺失，难以在同一技术栈内同时稳定“写入与检索”。
- Prisma 对 `vector(2048)` 无内置类型支持，需要 raw SQL；因此“写入”用 DAL + SQL 更稳，检索仍用 LlamaIndex 的运行时 API。
- 为了尽快交付业务闭环，采用“写入自研、检索 LlamaIndex”的折中设计，并通过严格的元数据与一致性约束减轻割裂带来的维护负担。

【取舍与 hack】

- 跳过横线分隔符、合并英文软换行，是针对团队现有文档的可控预处理，保证 `content` 干净可检索；
- `exactOptionalPropertyTypes: true` 下不显式传 `undefined`，对 `defaultCategory` 等字段做规范化或条件性赋值，保证编译期稳定；
- Neon 连接偶发超时：
  - 写库失败时脚本输出告警但不中断解析；
  - 建议使用 `sslmode=require`、检查 Compute 状态、必要时调高 `connectionTimeoutMillis` 或采用 `@neondatabase/serverless`；
  - 记录嵌入日志的 Prisma 写表若失败，允许继续向量写入与后续流程。

【效果验证】

- 单元测试：`12 files / 50 tests` 全通过；检索/写入相关测试覆盖元数据过滤与维度校验；
- 导入小样本：英文与中文文档均按章节分块，`content` 清洁无 `---`；`lang`/`category` 随章节注释绑定，首块不再缺失分类；
- 检索：`queryRag` 支持 `lang`/`category`/`is_public` 过滤，得分标准化为 `0..1` 并按降序返回。

【后续建议】

- 若未来需要统一到 LlamaIndex 全家桶：优先跟随官方 PGVectorStore 的稳定版本，再评估迁移成本；
- 保持分块器的“标题/段落/注释”三要素，在检索端构造更强的 prompt 与高质量上下文；
- 维持严格类型设置与参数条件赋值，以防止脚本演进引入未定义字段造成运行时脏数据。
