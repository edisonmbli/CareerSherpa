# Phase - 1

## 总体目标

- 使 M6（Upstash QStash + Upstash Redis）严格符合文档 2.2.3 与 2.3.0：按任务类型与付费/免费队列进行隔离、对模型维度做并发控制、在生产者侧做队列背压、工人侧严格释放计数与锁，并为 SSE 任务提供稳定的事件通道与超时规避。

## 现状回顾（代码指针）

- 生产者：`lib/queue/producer.ts` 统一推送到 `/api/worker/{stream|batch}/[service]`，未按 `QueueId` 分发（35, 94–105）。

- 守卫：`lib/worker/common.ts` 的 `enterGuards/exitGuards` 负责锁与背压（61–93），事件通道 `cs:events:{userId}:{serviceId}:{taskId}`（53–59）。

- 锁与计数器：`lib/concurrencyLock.ts`（8–19, 30–42）、`lib/redis/counter.ts`（63–101, 109–121）。

- 流式工人：`app/api/worker/stream/[service]/route.ts` 内联 LCEL 流，未复用 `lib/llm/service.ts:runStreamingLlmTask`。

- 队列路由表：`lib/llm/task-router.ts` 提供 `QueueId` 与 `modelId`，但生产者与工人未消费 `queueId` 做实际隔离。

- SSE 桥：`components/dev/SseStreamViewer.tsx` + `app/api/sse-stream/route.ts`（稳定心跳与 Redis Streams 缓冲）。

- 缺口：未接入真实 Server Actions（`uploadResumeAction/createServiceAction/...`）。

## 差距与修补要点

1. 队列分级隔离未落地：生产者与工人统一入口，未按 `QueueId`（付费/免费/视觉）拆路由。
2. 模型维度并发控制缺失：存在 `DEEPSEEK_MAX_WORKERS/GLM_MAX_WORKERS` 配置但未在守卫层按 `modelId`实施。
3. 背压位置偏离：当前由工人侧 `enterGuards` 进行；应在生产者侧入队前做 `bumpPending` 拦截。
4. 流式逻辑分散：工人路由内联 LCEL，未统一复用 `lib/llm/service.ts` 的流式执行入口。
5. 业务入口缺失：关键 Server Actions 未接通调度器，导致开发页能跑通，生产流未接入。

## 技术实施方案

### 阶段 1：路由与队列隔离

- 在生产者根据 `decision.queueId` 改造发布目标：将 QStash `url` 改为分层路径（如 `/api/worker/paid/stream/...`、`/api/worker/free/batch/...`、`/api/worker/paid/vision/...`）。

- 在 `app/api/worker/**` 下新增/整理路由结构，按 `paid/free/vision` + `stream/batch` 组合拆分入口。将 `enterGuards/exitGuards` 继续复用。

### 阶段 2：模型并发锁

- 在 `enterGuards` 前插入按 `modelId` 的并发锁：锁键示例 `lock:model:{modelId}:{tier}`（tier=paid/free）。

- 并发策略：`SET NX` 拿槽，并配合 `INCR` 计数与 `EXPIRE` TTL；达到阈值（来自 `ENV`）返回 `429` 并发布 `backpressure` 事件。

- 在 `exitGuards` 中释放模型并发计数与锁。

### 阶段 3：生产者侧背压

- 在 `lib/queue/producer.ts` 入队前调用 `bumpPending('queue:{queueId}', ttlSec, maxSize)`；超过阈值直接返回错误与 `Retry-After`。

- 将 `counterKey` 传入消息体，工人 `finally` 中 `decPending(counterKey)` 与释放锁保持一致。

### 阶段 4：统一流式执行入口

- 流式工人改用 `lib/llm/service.ts:runStreamingLlmTask`（保留 LCEL），事件发布统一走 `publishEvent` 合并批次与 Streams 写入，减少分散逻辑。

### 阶段 5：接通 Server Actions（M8/M9 前置）

- 新增并接通：`uploadResumeAction`、`uploadDetailedResumeAction`、`createServiceAction`（文本/图片分支）、`customizeResumeAction`、`generateInterviewTipsAction`。

- 每个 Action：`getOrCreateQuota` → 队列选择（付费/免费）→ 生产者背压检查 →（有配额则原子扣费）→ QStash 推送 → 立即返回 `taskId/isFree/stream`。

### 阶段 6：配置与一致性

- 将锁模块迁移或别名到 `lib/redis/lock.ts`（或更新文档）以保持一致。

- 落地 `ENV` 中的并发与队列大小引用：`getConcurrencyConfig()` 给守卫与生产者统一读取。

### 阶段 7：测试与验收

- 单元测试：

  - 背压：生产者超阈值返回 429 并不发布。

  - 并发锁：同模型并发超限被拒；释放后可继续。

  - 事件发布：`token_batch` 合并与 `done` 终止写 Streams。

- E2E（本地）：

  - 推 3 个 text（并发=1）串行、2 个 vision（并发=2）并行，计数器归零。

  - 一个付费、一个免费任务分别进入对应路由并正确限流。

- 开发页验证：`/(dev)/sse` 能在所有队列组合下正常流式与完成。

## 交付物

- 改造的生产者与按队列拆分的工人路由。

- 守卫层的模型并发锁与生产者侧队列背压。

- 统一流式执行入口与事件发布。

- 补齐的 Server Actions（带原子化扣费与降级标记）。

- 测试用例与本地 E2E 验收日志。

## 里程碑

- Milestone A（队列隔离+模型并发+生产者背压）：完成路由与守卫改造，跑通开发页。

- Milestone B（统一流层+Server Actions）：接通业务入口，开发页与页面流共同通过。

- Milestone C（测试与验收）：补全测试，完成 E2E 串并行、付费/免费分流与 SSE 稳定性验证。

## 风险与回避

- QStash 路由拆分后需要重新配置签名校验与 URL；通过共用 `verifySignatureAppRouter` 与严格路径校验避免误入。

- 并发锁需考虑工人异常退出的泄漏；使用 `EXPIRE TTL` 与 `finally` 兜底释放避免悬挂锁。

- 生产者背压与工人背压要保证计数一致；通过传递统一 `counterKey` 并在 `finally` 释放。

## 增补说明（M6 细化）

### 失败路径数据库更新与轮询对齐

- 流式 Worker 的 `catch`：`job_match` → 更新 `matches.status=FAILED`；`interview_prep` → 更新 `interviews.status=FAILED`。
- 批处理 Worker 的 `catch`：`job_summary` → 更新 `jobs.status=FAILED` 并联动 `matches.status=FAILED`；`resume_customize` → 更新 `customized_resumes.status=FAILED`。
- 轮询端点 `/api/task-status` 读取上述状态，前端即可展示失败并提示重试。

### `createServiceAction` 图片路径依赖提示

- 返回 `status='PENDING_OCR'`，并附带 `hint: { dependency: 'job_summary', next: ['STREAMING','COMPLETED'] }`，用于 UI 的依赖链引导。

### /(dev)/sse 测试页增强与验收

- 新增输入字段：`image`（触发 vision 路由）、`simulatePaid`（模拟付费队列）。
- Worker 支持 `variables.wasPaid` 覆盖路由决策，便于在开发页模拟付费/免费隔离。
- `start` 事件外显 `provider/modelId/queueId`，UI 显示用以验证队列隔离与模型选择。
- 验收清单：
  - 勾选与不勾选 `simulatePaid`，观察 `queueId` 分别落入 `q_paid_*` 与 `q_free_*`。
  - 填写 `image`，观察批处理路径 `start→done`；不填 `image`，观察流式路径 `token/token_batch→done`。
  - 不配置模型 Key 触发失败，观察失败事件与数据库 `status=FAILED`，轮询返回 `FAILED`。
  - 图片路径下 OCR 失败时，观察 `jobs.status=FAILED` 与 `matches.status=FAILED` 联动，前端状态从 `PENDING_OCR` 转为失败提示。

# Phase - 2

结论与方向

- 你的直觉是正确的：在 QStash 上维护“命名队列”并把 Worker 精简为 stream/batch 两类，是业界更成熟的做法，便于并行度控制、暂停/恢复、DLQ、观测等。当前实现是在应用层用多路由模拟“队列隔离”，队列概念更多地用于 Redis 计数与并发守卫，未使用 QStash 的“命名队列”能力。
- 我已整理出迁移与精简的完整计划，并将你的补充要求融合进来。下面是针对你的三个问题的深入分析与落地方案。
  1）QStash 队列 vs Worker 路由

- 设计对齐：
  - 方案要求“按体验模式 + 付费等级划分队列”，最佳落地是“QStash 命名队列 + 两类 Worker”。也就是队列层面实现隔离与并行度控制，Worker 层保持统一、只做签名校验与业务执行。
- 当前实现的差异：
  - 多路由模拟队列隔离：存在 6 个入口路由（paid/free/vision × stream/batch），但本质上逻辑集中在 lib/worker/handlers.ts 两个处理器中；“队列”只以 queueId 用于 Redis 计数（ lib/queue/producer.ts:145-164 ），并未在 QStash 创建“命名队列”与并行度管控。
  - Producer 发布时只指定 url （分层路径）；未使用 QStash 队列名（ lib/queue/producer.ts:192-205 ）。
- 结论：

  - 最佳实践是用 QStash 的“命名队列”，把并发控制前移到队列层；Worker 路由精简为 stream/batch 两类（视觉仍归 batch ），减少路由套壳与开发态复杂度。
  - 现状可运行，但确实“像自己闭门造工具”；迁移到 QStash 多队列能提升运维可视性与可控性。
    2）多队列落地最佳实践与与当前实现的 gap

- 多队列最佳实践：
  - 在 QStash Console 创建 6 个命名队列（ q_paid_stream / q_free_stream / q_paid_batch / q_free_batch / q_paid_vision / q_free_vision ），设置并行度（并行度是队列层粗粒度门控）。
  - Producer 在 publishJSON 指定消息的 queue ；同时把 queueId/tier/modelId 放入消息体，供 Worker 做模型级/用户级精细守卫与审计。
  - Worker 统一入口（ stream/batch ），在处理器内部依据 queueId/tier 执行模型并发锁、用户并发锁、Redis 背压、SSE 发布。
  - QStash 的队列并行度与暂停/恢复、DLQ，承担“流量闸门”的职责；Redis 守卫承担“模型/用户”维度的细粒度控制与一致性清理。
- 与现在的 gap：
  - 队列层：当前未使用 QStash 多队列能力，只用 URL 分层与 Redis 计数；缺失队列级并行度、暂停、DLQ。
  - 路由层：维护 6 个路由，增加开发态复杂度；迁移后保留 2 个主入口。
- 孰优孰劣：
  - 多队列优：更强的队列可控与观测，减少代码复杂度；运维更易。
  - 现实现优：无需队列配置；但维护成本高，且缺少队列级治理。
- 切换需要调整：

  - Producer 增加 queue 指定（保留消息体中的 queueId ）。
  - 渐进式下线路由适配器，保留兼容期。
  - 保留 Redis 的模型并发锁与用户并发锁，成为“细粒度兜底”。
    3）结构重构空间与具体建议

- 处理器管线化：
  - 将 lib/worker/handlers.ts 拆分为“步骤函数”：参数验证、路由决策（含 tierOverride/wasPaid/真实配额 ）、守卫（用户/模型/队列）、执行（调用 runStreamingLlmTask / runStructuredLlmTask ）、事件发布、清理。
  - 每步为纯函数、统一返回 Result<T,E> ，聚合在一个 orchestrator 中；提升可读性与测试友好性。
- 契约与类型：
  - 抽出 WorkerBody / WorkerEvent union 类型到 lib/worker/types.ts ，用 Zod 进行入参与事件校验与推断。
  - 抽出并发/队列配置到 lib/config/concurrency.ts ，减少 ENV 散读。
- 可观测性：
  - 统一 trackEvent / auditUserAction 到 lib/observability/logger.ts ，结构化输出，增加采样与 traceId 注入。
- 测试与脚本：
  - 增加 scripts/dev/worker-sim.ts 离线压测脚本，直接调用处理器（绕过 App Router），覆盖三态队列覆盖、守卫失败、Provider 未配置、限时失败等路径。
- 路由适配器治理：

  - 保留但不相互 import，签名后直接调用处理器；最终通过 URL Group 或重定向引导统一入口，逐步减少重复路由。
    你提出的四点补充已融合

- 并发与重试策略（关于 requeueWithDelay）：
  - 既然队列并行度能控制并发，建议移除“应用侧的限时内重试入队”（ lib/worker/common.ts:79-98 及处理器中的 requeueWithDelay 调用），改为：守卫失败直接返回相应错误（429/503），由 QStash 的 retries 策略驱动重试（ lib/queue/producer.ts:199 已设置 retries: 3 ）。这样避免“自旋 requeue”，把节流交回队列层。
  - 保留 Redis 背压计数，作为附加的保护与审计来源（包括拒绝与 Retry-After 指示）。
- Free plan 并行度=2 与模型并发锁取舍：
  - 建议保留模型并发锁作为“细粒度兜底”，原因：队列并行度控制的是队列总并发，而模型 API 限制可能按模型/账户维度；即使队列并行度放大，模型锁能防止越过 Provider 的限制。
  - 随着付费提升队列并行度，模型锁参数可动态调整；但不建议全移除（除非在观察期确认 Provider 限制足够宽松）。
- DeepSeek“服务器繁忙 keep-alive/空行”处理：
  - 流式：当前 runStreamingLlmTask 只在 text 非空时发布 token（ lib/llm/service.ts:351-357 ），SSE keep-alive 注释一般不会进入 text ，因此不会误发布，连接会保持直到内容产生或超时（ timeoutMs: ENV.WORKER_TIMEOUT_MS 已设置于 service.ts:345 ）。这覆盖了 keep-alive 情况。
  - 非流式：若返回空行，当前链是一次性 invoke ，会受 timeoutMs 保护，超时后走错误分支并发布 error 事件；不会无尽等待。
  - 建议增加“流式空闲心跳事件”与“非流式超时的特定错误码”，提升可观测性；功能不变，仅便于运维判读。
- 代码工程的整体精简：

  - 不仅 Worker，还包括 Producer（路由决策与背压）、并发/队列配置（统一配置源）、日志与审计（统一模块），以项目级工程规范推进。
    执行里程碑（按你的分步要求）

- 里程碑 A：迁移到 QStash 多队列；移除应用侧 requeue；补齐 DeepSeek 繁忙期处理标识（保持现有超时与心跳策略，增加事件码与日志）；
- 验收 A：你人工测试三态覆盖（Auto/Paid/Free）与视觉路径；观察 queueId 与事件序列；对比 QStash 队列监控与并行度效果。
- 里程碑 B：调度器架构精简（处理器管线化、类型与契约、配置集中化、日志统一、脚本压测）。
- 验收 B：你人工二次测试，确保功能不变、观测更清晰、代码更易读。

---

**现状分析**

- 队列语义在代码中以 `QueueId` 表示，但并未在 QStash 创建对应“命名队列”；生产者通过 `url` 分层路径把消息推到多个 API 路由，同时用 Redis 计数做背压（`lib/queue/producer.ts:140-159`）。

- Worker 入口按“体验模式+付费等级”拆成 6 个路由，逻辑集中在两个核心处理器（`lib/worker/handlers.ts`），路由仅做签名与转发。

- SSE 与并发守卫在处理器内完成，运行稳定，但存在路由重复与开发态维护成本。

**最佳实践建议（QStash 多队列）**

- 在 QStash Console 创建 6 个“命名队列”（`q_paid_stream`、`q_free_stream`、`q_paid_batch`、`q_free_batch`、`q_paid_vision`、`q_free_vision`），为每队列设置并行度（与业务期望一致，具体策略见 Solution Spec）。

- 生产者推送消息时显式指定 `queue`（QStash 支持按队列分发与并行度控制），同时仍传 `queueId` 到消息体用于 Worker 侧的细粒度守卫与审计。

- Worker 路由精简为 2 个入口：`/api/worker/stream/[service]` 与 `/api/worker/batch/[service]`（视觉仍走 `batch`），避免“路由互相导入”与重复包装器；6 个适配器路由可保留一版（向后兼容）或逐步下线。

- 并发控制分层：

  - 队列层（QStash）：利用“命名队列”的并行度进行粗粒度门控与暂停/恢复。

  - 模型层（Redis）：保留现有 `enterModelConcurrency/exitModelConcurrency` 针对 `modelId+tier` 做限流（`lib/worker/common.ts:123-129`）。

  - 用户层（Redis）：保留现有 `enterUserConcurrency/exitUserConcurrency`。

- 生产者侧背压保留（Redis 计数），与 QStash 队列并行度共同形成双保险；SSE 与事件桥逻辑不变。

**与当前实现的差异与优劣**

- 当前：通过分层 URL 模拟队列隔离，未使用 QStash“命名队列”，维护 6 个路由增加开发态复杂度；优点是无需外部队列配置即可隔离；缺点是缺乏队列级暂停、并行度与可视化管理。

- 建议方案：真正用 QStash 多队列，生产者按队列名推送，Worker 统一入口；优点是运维友好、并行度与暂停可视、降低路由复杂度；缺点是需要在 QStash Console/SDK 进行队列配置与权限管理。

**迁移步骤**

1. 在 QStash Console 创建 6 个队列并设置并行度（与 `docs/2.Solution_Spec.md:90-133` 对齐）。
2. 更新生产者：在 `publishJSON` 指定 `queue` 字段/头（与 QStash SDK 对齐），同时消息体保留 `queueId`、`tier`、`modelId` 以便 Worker 侧守卫与审计。
3. 精简 Worker 路由：保留核心 2 个入口，6 个适配器路由改为薄层（签名校验后转发到处理器或直接返回“使用统一入口”的兼容提示），逐步下线。
4. 校验并发与背压：

   - 队列级：在 QStash 调整并行度验证吞吐与暂停行为。

   - 模型级/用户级：保持 Redis 锁与计数器逻辑不变，确保 `exit` 路径覆盖异常。

5. 验收与监控：

   - SSE 测试页：三态覆盖（Auto/Paid/Free）分别验证 `queueId` 与事件序列。

   - QStash 队列监控：观察每队列流量与并行度；必要时启用 DLQ。

**进一步重构建议（不改功能，仅优化结构）**

- 处理器拆分为“管线步骤”：验证 → 路由决策 → 守卫 → 执行 → 事件 → 清理，每步为纯函数：`lib/worker/steps/*`，通过组合驱动，便于单测与维护。

- 类型与契约：为消息体与事件定义 `zod`/TS 类型（`WorkerBody`、`WorkerEvent` union），集中在 `lib/worker/types.ts`；引入 `Result<T,E>` 以明确错误分支。

- 配置集中化：并发、队列大小、模型阈值统一到 `lib/config/concurrency.ts`，减少散落读取。

- 日志与审计统一：`lib/observability/logger.ts` 封装 `trackEvent/audit`，支持结构化字段与采样。

- 测试与脚本：新增 `scripts/dev/worker-sim.ts`，直接调用处理器模拟请求，便于离线压测与 CI；完善单测覆盖各管线步骤。

- 路由适配器治理：保留但不相互 import；后续通过 URL 组或 301 指引统一入口，避免再发生递归堆栈与 AsyncHooks 膨胀。

**交付与验证**

- 先完成生产者与 QStash 队列接入的最小改动，保持现有 Redis 守卫与处理器逻辑不变，逐步下线路由套壳。

- 提供验收清单：三态 SSE、付费/免费/视觉三组合、并行度测试、队列暂停/恢复、DLQ 验证、模型并发锁触顶与释放。

## Worker & Queue Architecture (M6 Refactor Best Practices)

- QStash 多队列（命名队列）

  - 必须在 QStash Console 创建 6 个命名队列：`q_paid_stream/q_free_stream/q_paid_batch/q_free_batch/q_paid_vision/q_free_vision`，并为每队列设置并行度（Parallelism）。
  - 生产者发布时必须显式指定队列：使用 `client.queue({ queueName }).enqueueJSON({ url, body, retries })`，而不是默认 `publishJSON`。
  - 生产者统一入口 URL：`/api/worker/{stream|batch}/[service]`（视觉任务映射到 `batch`），消息体同时包含 `queueId` 以便工人侧审计与守卫。
  - 队列选择优先级：`tierOverride → wasPaid → 真实配额`（统一于生产者与工人）。实现参考 `lib/queue/producer.ts:43-50, 192-206` 与 `lib/worker/handlers.ts:62-72`。

- Worker 路由与职责

  - 仅保留两个核心路由入口：`/api/worker/stream/[service]` 与 `/api/worker/batch/[service]`（App Router）。
  - 路由必须保持“薄”，只做签名校验（`verifySignatureAppRouter`）、参数解析与调用处理器，不得引入业务复杂逻辑。参考 `app/api/worker/stream/[service]/route.ts`、`app/api/worker/batch/[service]/route.ts`。
  - 禁止路由之间相互 import，避免开发态递归与 AsyncHooks 膨胀。

- 处理器管线化（纯函数步骤）

  - 步骤划分与目录规范：
    - 验证：`lib/worker/types.ts`（Zod schema），`parseWorkerBody` 在 `lib/worker/common.ts:19-35`。
    - 决策：`lib/worker/steps/decision.ts:1-13`（`computeDecision(templateId, variables, userHasQuota)`）。
    - 元信息：`lib/worker/steps/meta.ts:1-10`（`getRequestMeta(req, taskId)` 统一生成 `requestId/traceId`）。
    - 守卫：`lib/worker/steps/guards.ts:1-66`（`guardUser/guardModel/guardQueue`），失败统一通过 `guardBlocked` 发事件。
    - 执行：`lib/worker/steps/execute.ts:1-35`（`executeStreaming/executeStructured` 返回统计信息）。
    - 事件：`lib/worker/pipeline.ts:1-67`（`publishStart/guardBlocked/emitStreamIdle`）。
    - 清理：`lib/worker/steps/cleanup.ts:1-13`（`cleanupFinal` 统一退出并发与背压计数）。
  - 处理器文件只按顺序调用上述步骤，行为保持不变；参考 `lib/worker/handlers.ts`。

- 并发与背压（分层控制）

  - 队列层（QStash）：使用命名队列的并行度作为粗粒度闸门，支持暂停/恢复与 DLQ。生产者端的应用层重试（如 `requeueWithDelay`）已移除，统一依赖 QStash 的 `retries`。
  - 模型层（Redis）：按 `modelId+tier` 控制并发。
    - 键构造与阈值统一在 `lib/config/concurrency.ts:15-35`（`buildModelActiveKey/getMaxWorkersForModel`）。
    - 进入/退出在 `lib/worker/common.ts:176-201`。
  - 用户层（Redis）：按用户维度限制并发。
    - 键构造在 `lib/config/concurrency.ts:37-40`（`buildUserActiveKey`），使用于 `lib/worker/common.ts:207-216`。
  - 队列背压计数：队列大小读取统一在 `lib/config/concurrency.ts:1-13`（`queueMaxSizeFor`）；构造键在 `lib/config/concurrency.ts:42-44`（`buildQueueCounterKey`）。

- 日志与审计统一

  - 使用 `lib/observability/logger.ts` 的 `logEvent/logAudit`，禁止在业务中直接调用 `trackEvent/auditUserAction`。
  - 事件名必须为 `AnalyticsEventName`，统一字段 `{ userId, serviceId, taskId, payload }`；参考 `lib/queue/producer.ts` 与 `lib/worker/handlers.ts` 中的替换实现。

- SSE 与可观测性

  - `start` 事件必须包含 `queueId/provider/modelId/requestId/traceId`，Viewer 顶部需显示队列与模型信息。参考 `lib/worker/pipeline.ts` 与 `components/dev/SseStreamViewer.tsx:96-101, 136-142`。
  - 流式空闲心跳：长时间无 token 时发布 `info: stream_idle` 事件，用于高负载期排查（DeepSeek keep-alive 场景）。

- 配置集中化

  - 并发与队列大小读取、键构造函数集中于 `lib/config/concurrency.ts`，避免分散常量与重复逻辑。
  - 生产者与工人均应依赖该集中入口，减少参数不一致风险。

- 开发与压测
  - 离线模拟脚本：`scripts/dev/worker-sim.ts` 支持 `stream|batch free|paid job_match` 的本地压测，不依赖页面即可验证处理器行为与守卫路径。
